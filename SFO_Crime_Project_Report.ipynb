{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#San Francisco Crime Prediction \n",
    "\n",
    "We are involved in the kaggle.com Crime Prediction contest. We are using the data provided about crimes that occurred in San Francisco from 2011 to 2015. The data are variations on time and location, and the labels on the training set are 39 different crimes. \n",
    "\n",
    "To solve this problem we use various models and feature engineering. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Initial Attempt\n",
    "\n",
    "To get on the leaderboard, we did basic feature extraction and normalization, and used a logistric regression model. \n",
    "\n",
    "###Feature Engineering Initial Summary\n",
    "\n",
    "* Semi-normalize years: subtract lowest year, 2011, so that year numbers are from 0 to 4. \n",
    "* Separate time information into year, month, day, time of day, day of week\n",
    "* Normalize latitude and longitude (use (value - mean)/standard deviation)\n",
    "\n",
    "The results of this attempt are summarized here: \n",
    "\n",
    "-<<<add markdown from README.md >>>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Feature Engineering: Going Deeper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Variations on Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Ensemble Approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Results, Error Analysis, Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/Users/marjoriesayer/anaconda/envs/mids_hw1/lib/python2.7/site-packages')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# General Libraries\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "# SK-learn libraries for learning.\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "# SK-learn libraries for evaluation.\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# SK-learn libraries for feature extraction from text.\n",
    "from sklearn.feature_extraction.text import *\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.metrics import log_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the Crime data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Dates', 'Descript', 'DayOfWeek', 'PdDistrict', 'Resolution', 'Address', 'X', 'Y']\n"
     ]
    }
   ],
   "source": [
    "crime_data = []\n",
    "crime_labels = []\n",
    "with open(\"../../train.csv\", \"rb\") as csvfile:\n",
    "    reader = csv.reader(csvfile, delimiter=',')\n",
    "    first_row = True\n",
    "    for row in reader:\n",
    "        data = [y for x, y in enumerate(row) if x != 1]\n",
    "        name = [y for x, y in enumerate(row) if x == 1]\n",
    "        \n",
    "        if first_row:\n",
    "            first_row = False\n",
    "            feature_names = data\n",
    "            print feature_names\n",
    "        else:\n",
    "            crime_data.append(data)\n",
    "            crime_labels.append(name)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Divide the Crime data into Train data, test data, mini train data, and dev sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "378049\n",
      "Size of train data:  500000\n",
      "Size of dev data:  189024\n",
      "Size of test data:  189025\n"
     ]
    }
   ],
   "source": [
    "# Divide the Crime training set into train data, test data, dev data\n",
    "train_data, train_labels = crime_data[:500000], crime_labels[:500000]\n",
    "crime_test, crime_test_labels = crime_data[500000:], crime_labels[500000:]\n",
    "num_test = len(crime_test)\n",
    "print num_test\n",
    "dev_data, dev_labels = crime_test[:num_test/2], crime_test_labels[:num_test/2]\n",
    "test_data, test_labels = crime_test[num_test/2:], crime_test_labels[num_test/2:]\n",
    "mini_train_data, mini_train_labels = crime_data[200000:300000], crime_labels[200000:300000]\n",
    "\n",
    "print \"Size of train data: \", len(train_data)\n",
    "print \"Size of dev data: \", len(dev_data)\n",
    "print \"Size of test data: \", len(test_data)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The next section has all UTIL functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2015-05-13 23:53:00\n",
      "[2015, 5, 13, 1433.0, 23, 'Night']\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "def convert_date(date_val):\n",
    "    date_list = []\n",
    "    date, time = date_val.split()\n",
    "    date_list.append(int(date.split('-')[0]))\n",
    "    date_list.append(int(date.split('-')[1]))\n",
    "    date_list.append(int(date.split('-')[2]))\n",
    "    time_in_min = int(time.split(':')[0]) * 60.0 + int(time.split(':')[1])\n",
    "    date_list.append(time_in_min)\n",
    "    date_list.append(int(time.split(':')[0]))\n",
    "    time_hour = int(time.split(':')[0])\n",
    "    if time_hour < 6:\n",
    "        time_of_day = 'Twilight'\n",
    "    elif time_hour < 12:\n",
    "        time_of_day = 'Morning'\n",
    "    elif time_hour < 18:\n",
    "        time_of_day = 'Afternoon'\n",
    "    else:\n",
    "        time_of_day = 'Night'\n",
    "    date_list.append(time_of_day)\n",
    "    return date_list\n",
    "\n",
    "print train_data[0][0]\n",
    "print convert_date(train_data[0][0])\n",
    "\n",
    "def find_mean_std(train_data, index):\n",
    "    data = []\n",
    "    for row in train_data:\n",
    "        data.append(float(row[index]))\n",
    "    \n",
    "    data_arr = np.array(data, dtype=np.float32)\n",
    "    print np.mean(data_arr)\n",
    "    data_dict = {}\n",
    "    data_dict['mean'] = np.mean(data_arr)\n",
    "    data_dict['std'] = np.std(data_arr)\n",
    "    return data_dict"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Initialized Arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "districts = ['NORTHERN', 'PARK', 'INGLESIDE', 'BAYVIEW', 'RICHMOND', 'CENTRAL', 'TARAVAL', 'TENDERLOIN', 'MISSION', 'SOUTHERN']\n",
    "day = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "daytime = ['Twilight', 'Morning', 'Afternoon', 'Night']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Engineering - Section I:\n",
    "\n",
    "1) Crime Date - Normalization\n",
    "\n",
    "2) Longitude & Latitude - Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-122.423\n",
      "37.7675\n"
     ]
    }
   ],
   "source": [
    "''' Crime Date - Normalization '''\n",
    "\n",
    "year_data = []\n",
    "mon_data = []\n",
    "day_data = []\n",
    "time_data = []\n",
    "time_of_day_data = []\n",
    "for row in crime_data:\n",
    "    date_arr = convert_date(row[0])\n",
    "    year_data.append(float(date_arr[0]))\n",
    "    mon_data.append(float(date_arr[1]))\n",
    "    day_data.append(float(date_arr[2]))\n",
    "    time_data.append(float(date_arr[3]))\n",
    "    time_of_day_data.append(date_arr[5])\n",
    "                     \n",
    "\n",
    "year_arr = np.array(year_data, dtype=np.float32)\n",
    "mon_arr = np.array(mon_data, dtype=np.float32)\n",
    "day_arr = np.array(day_data, dtype=np.float32)\n",
    "time_arr = np.array(time_data, dtype=np.float32)\n",
    "time_of_day_arr = np.array(time_of_day_data)\n",
    "\n",
    "date_dict = {}\n",
    "date_dict['mean_year'] = np.mean(year_arr)\n",
    "date_dict['std_year'] = np.std(year_arr)                               \n",
    "date_dict['mean_mon'] = np.mean(mon_arr)\n",
    "date_dict['std_mon'] = np.std(mon_arr)\n",
    "date_dict['mean_day'] = np.mean(day_arr)\n",
    "date_dict['std_day'] = np.std(day_arr)\n",
    "date_dict['mean_time'] = np.mean(time_arr)\n",
    "date_dict['std_time'] = np.std(time_arr)\n",
    "\n",
    "\n",
    "''' Longitude & Latitude - Normalization '''\n",
    "long_dict = find_mean_std(train_data, 6)\n",
    "lat_dict = find_mean_std(train_data, 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  2.01500000e+03   5.00000000e+00   1.30000000e+01   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   1.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   1.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   1.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    1.29296839e-01   2.97121316e-01   0.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def get_formatted_data(train_data):\n",
    "    format_data = []\n",
    "    count = 0\n",
    "    \n",
    "    for row in train_data:\n",
    "        count += 1\n",
    "        data = []\n",
    "        \n",
    "        ''' Data Preparation - Crime date '''\n",
    "        \n",
    "        #date_list = convert_date(row[0])\n",
    "        date_arr = convert_date(row[0])\n",
    "        \n",
    "        data.extend(date_arr[:3]) # Normalized date\n",
    "        \n",
    "        # Time of the day preparation\n",
    "        time_of_day = date_arr[5]\n",
    "        day_time = [1 if time_of_day == t else 0 for t in daytime]\n",
    "        data.extend(day_time)\n",
    "        \n",
    "        #year_val = (float(date_list[0]) - date_dict['mean_year'])/date_dict['std_year']\n",
    "        #data.append(year_val)\n",
    "        #mon_val = (float(date_list[1]) - date_dict['mean_mon'])/date_dict['std_mon']\n",
    "        #data.append(mon_val)\n",
    "        #day_val = (float(date_list[2]) - date_dict['mean_day'])/date_dict['std_day']\n",
    "        #data.append(day_val)\n",
    "        #time_val = (float(date_list[3]) - date_dict['mean_time'])/date_dict['std_time']\n",
    "        #data.append(time_val)\n",
    "        \n",
    "        ''' Data Preparation - Day of the Week '''\n",
    "        week_day = row[2]\n",
    "        crime_day = [1 if week_day == d else 0 for d in day] \n",
    "        data.extend(crime_day) # Normalized\n",
    "        \n",
    "        ''' Data Preparation - District '''\n",
    "        pdd = row[3]\n",
    "        pddistrict = [1 if pdd == d else 0 for d in districts] # Normalized\n",
    "        data.extend(pddistrict)\n",
    "        \n",
    "        ''' Data Preparation - Longitude & Latitude '''\n",
    "        longitude = float(row[6])\n",
    "        long_norm = (abs(longitude) - abs(long_dict['mean']))/long_dict['std'] # Normalized\n",
    "        latitude = float(row[7])\n",
    "        lat_norm = (abs(latitude) - abs(lat_dict['mean']))/lat_dict['std'] # Normalized\n",
    "        data.append(long_norm)\n",
    "        data.append(lat_norm)\n",
    "        \n",
    "        ''' Data Preparation - Address '''\n",
    "        \n",
    "        address = row[5].lower()\n",
    "        addr = [1 if 'block' in address else 0] # Requires major improvement\n",
    "        data.extend(addr)\n",
    "        \n",
    "        # Explore external map source\n",
    "        \n",
    "        ''' Quantify the data '''\n",
    "        format_data.append(np.array(data, dtype=np.float32))           \n",
    "        \n",
    "    return format_data\n",
    "    \n",
    "    \n",
    "crimeX = np.array(get_formatted_data(crime_data), dtype=np.float32)\n",
    "trainX = np.array(get_formatted_data(train_data), dtype=np.float32)\n",
    "testX = np.array(get_formatted_data(test_data), dtype=np.float32)\n",
    "devX = np.array(get_formatted_data(dev_data), dtype=np.float32)\n",
    "mini_trainX = np.array(get_formatted_data(mini_train_data), dtype=np.float32)\n",
    "print trainX[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CrimeX Data shape:  (878049, 27)\n",
      "CrimeX Label shape:  (878049, 1)\n",
      "testX Data shape:  (189025, 27)\n",
      "testX Label shape: (189025, 1)\n",
      "Mini TrainX Data shape:  (100000, 27)\n",
      "Mini TrainX Label shape:  (100000, 1)\n"
     ]
    }
   ],
   "source": [
    "print \"CrimeX Data shape: \", crimeX.shape\n",
    "print \"CrimeX Label shape: \", np.array(crime_labels).shape\n",
    "print \"testX Data shape: \", testX.shape\n",
    "print \"testX Label shape:\", np.array(test_labels).shape\n",
    "print \"Mini TrainX Data shape: \", mini_trainX.shape\n",
    "print \"Mini TrainX Label shape: \", np.array(mini_train_labels).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_labels = np.array(train_labels).ravel()\n",
    "test_labels = np.array(test_labels).ravel()\n",
    "crime_labels = np.array(crime_labels).ravel()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Building the Logistic Regression model on the entire Crime Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed the training\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(C=0.1)\n",
    "lr.fit(crimeX, crime_labels)\n",
    "print \"Completed the training\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.320155666949\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/marjoriesayer/anaconda/envs/mids_hw1/lib/python2.7/site-packages/sklearn/metrics/classification.py:676: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=\"f1_weighted\" instead of scoring=\"f1\".\n",
      "  sample_weight=sample_weight)\n",
      "/Users/marjoriesayer/anaconda/envs/mids_hw1/lib/python2.7/site-packages/sklearn/metrics/classification.py:960: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "mini_testX, mini_test_labels = testX[:10000], test_labels[:10000]\n",
    "f1_score = metrics.f1_score(lr.predict(mini_testX), mini_test_labels)\n",
    "print f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "f1 score when no regularization: 0.306096502629\n",
    "with regularization of 0.1: 0.320155666949\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'f1_score' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-a6547053d37c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mprint\u001b[0m \u001b[0mf1_score\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'f1_score' is not defined"
     ]
    }
   ],
   "source": [
    "print f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sub_test_data = []\n",
    "with open(\"../../test.csv\", \"rb\") as csvfile:\n",
    "    reader = csv.reader(csvfile, delimiter=',')\n",
    "    first_row = True\n",
    "    for row in reader:\n",
    "        data = [y for x, y in enumerate(row)]\n",
    "        if first_row:\n",
    "            first_row = False\n",
    "        else:\n",
    "            sub_test_data.append(data)\n",
    "            \n",
    "\n",
    "def get_formatted_test_data(test_data):\n",
    "    format_data = []\n",
    "    count = 0\n",
    "    \n",
    "    for row in test_data:\n",
    "        count += 1\n",
    "        data = []\n",
    "        \n",
    "        date_arr = convert_date(row[1])\n",
    "        data.extend(date_arr[:3]) # Normalized date\n",
    "        time_of_day = date_arr[5]\n",
    "        day_time = [1 if time_of_day == t else 0 for t in daytime]\n",
    "        data.extend(day_time)\n",
    "        \n",
    "        #year_val = (float(date_arr[0]) - date_dict['mean_year'])/date_dict['std_year']\n",
    "        #data.append(year_val)\n",
    "        #mon_val = (float(date_arr[1]) - date_dict['mean_mon'])/date_dict['std_mon']\n",
    "        #data.append(mon_val)\n",
    "        #mon_val = (float(date_arr[2]) - date_dict['mean_day'])/date_dict['std_day']\n",
    "        #data.append(day_val)\n",
    "        #time_val = (float(date_arr[3]) - date_dict['mean_time'])/date_dict['std_time']\n",
    "        #data.append(time_val)\n",
    "        \n",
    "        week_day = row[2]\n",
    "        crime_day = [1 if week_day == d else 0 for d in day] \n",
    "        data.extend(crime_day)\n",
    "        \n",
    "        pdd = row[3]\n",
    "        pddistrict = [1 if pdd == d else 0 for d in districts]\n",
    "        data.extend(pddistrict)\n",
    "        \n",
    "        longitude = float(row[5])\n",
    "        long_norm = (abs(longitude) - abs(long_dict['mean']))/long_dict['std']\n",
    "        latitude = float(row[6])\n",
    "        lat_norm = (abs(latitude) - abs(lat_dict['mean']))/lat_dict['std']\n",
    "        data.append(long_norm)\n",
    "        data.append(lat_norm)\n",
    "        \n",
    "        if 'block' in row[4].lower():\n",
    "            address = 1\n",
    "        else:\n",
    "            address = 0\n",
    "        data.append(address)\n",
    "        format_data.append(np.array(data, dtype=np.float32))           \n",
    "        \n",
    "    return format_data\n",
    "    \n",
    "    \n",
    "\n",
    "testX = np.array(get_formatted_test_data(sub_test_data), dtype=np.float32)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "print \"hello\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "probs = lr.predict_proba(testX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(884262, 39)\n",
      "[[  5.08778624e-03   1.10662398e-01   9.17294038e-05   6.67460252e-04\n",
      "    4.91912604e-02   1.85602845e-03   1.91787501e-03   3.90408942e-02\n",
      "    4.46866918e-03   4.70025175e-04   1.49900134e-04   6.37088109e-04\n",
      "    3.42208852e-03   6.76801238e-03   1.73835233e-04   4.58723724e-03\n",
      "    1.57442886e-01   1.40903942e-03   2.30785097e-04   5.65454880e-02\n",
      "    7.59824096e-02   1.13616602e-01   1.59773416e-05   7.68972530e-04\n",
      "    4.28024260e-03   2.55923180e-02   3.81613786e-03   2.63796327e-02\n",
      "    4.98827057e-03   2.34718363e-04   3.72114962e-03   4.43382191e-04\n",
      "    3.99266692e-02   6.08285202e-06   6.19009855e-03   8.00325729e-02\n",
      "    1.06607402e-01   4.35350999e-02   1.90417739e-02]]\n"
     ]
    }
   ],
   "source": [
    "print probs.shape\n",
    "print probs[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job completed\n"
     ]
    }
   ],
   "source": [
    "import gzip\n",
    "with gzip.open('submission-matrix_m1.csv.gz', 'wb') as f:\n",
    "    out = csv.writer(f, lineterminator='\\n')\n",
    "    out.writerow(['Id'] + list(np.unique(crime_labels)))\n",
    "    \n",
    "    for i, prob in enumerate(probs):\n",
    "        out.writerow([i] + list(prob))\n",
    "print \"Job completed\"     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Training Completed\n"
     ]
    }
   ],
   "source": [
    "clf = RandomForestClassifier(n_estimators=39)\n",
    "clf.fit(crimeX, crime_labels)\n",
    "print \"Random Forest Training Completed\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.18963076261\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/marjoriesayer/anaconda/envs/mids_hw1/lib/python2.7/site-packages/sklearn/metrics/classification.py:676: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=\"f1_weighted\" instead of scoring=\"f1\".\n",
      "  sample_weight=sample_weight)\n",
      "/Users/marjoriesayer/anaconda/envs/mids_hw1/lib/python2.7/site-packages/sklearn/metrics/classification.py:958: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "mini_testX, mini_test_labels = testX[:10000], test_labels[:10000]\n",
    "f1_score = metrics.f1_score(clf.predict(mini_testX), mini_test_labels)\n",
    "print f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(884262, 39)\n",
      "[[ 0.01709402  0.18437118  0.          0.          0.01648352  0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.28205128  0.          0.          0.05128205\n",
      "   0.10683761  0.07692308  0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.02564103  0.08547009\n",
      "   0.          0.          0.05128205  0.05128205  0.02564103  0.02564103]]\n"
     ]
    }
   ],
   "source": [
    "clf_probs = clf.predict_proba(testX)\n",
    "print clf_probs.shape\n",
    "print clf_probs[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gzip\n",
    "with gzip.open('submission-matrix.csv.gz', 'wb') as f:\n",
    "    out = csv.writer(f, lineterminator='\\n')\n",
    "    out.writerow(['Id'] + list(np.unique(crime_labels)))\n",
    "    \n",
    "    for i, prob in enumerate(clf_probs):\n",
    "        out.writerow([i] + list(prob))\n",
    "print \"Job completed\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
