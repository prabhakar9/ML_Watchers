{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# General Libraries\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "# SK-learn libraries for learning.\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "# SK-learn libraries for evaluation.\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# SK-learn libraries for feature extraction from text.\n",
    "from sklearn.feature_extraction.text import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the Crime data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Dates', 'Descript', 'DayOfWeek', 'PdDistrict', 'Resolution', 'Address', 'X', 'Y']\n"
     ]
    }
   ],
   "source": [
    "crime_data = []\n",
    "crime_labels = []\n",
    "with open(\"../train.csv\", \"rb\") as csvfile:\n",
    "    reader = csv.reader(csvfile, delimiter=',')\n",
    "    first_row = True\n",
    "    for row in reader:\n",
    "        data = [y for x, y in enumerate(row) if x != 1]\n",
    "        name = [y for x, y in enumerate(row) if x == 1]\n",
    "        \n",
    "        if first_row:\n",
    "            first_row = False\n",
    "            feature_names = data\n",
    "            print feature_names\n",
    "        else:\n",
    "            crime_data.append(data)\n",
    "            crime_labels.append(name)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Divide the Crime data into Train data, test data, mini train data, and dev sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "378049\n",
      "Size of train data:  500000\n",
      "Size of dev data:  189024\n",
      "Size of test data:  189025\n"
     ]
    }
   ],
   "source": [
    "# Divide the Crime training set into train data, test data, dev data\n",
    "train_data, train_labels = crime_data[:500000], crime_labels[:500000]\n",
    "crime_test, crime_test_labels = crime_data[500000:], crime_labels[500000:]\n",
    "num_test = len(crime_test)\n",
    "print num_test\n",
    "dev_data, dev_labels = crime_test[:num_test/2], crime_test_labels[:num_test/2]\n",
    "test_data, test_labels = crime_test[num_test/2:], crime_test_labels[num_test/2:]\n",
    "mini_train_data, mini_train_labels = crime_data[200000:300000], crime_labels[200000:300000]\n",
    "\n",
    "print \"Size of train data: \", len(train_data)\n",
    "print \"Size of dev data: \", len(dev_data)\n",
    "print \"Size of test data: \", len(test_data)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The next section has all UTIL functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2015-05-13 23:53:00\n",
      "[2015, 5, 13, 1433.0]\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "def convert_date(date):\n",
    "    date_list = []\n",
    "    date_val = time.strptime(date, '%Y-%m-%d %H:%M:%S')\n",
    "    date_list.append(date_val.tm_year)\n",
    "    date_list.append(date_val.tm_mon)\n",
    "    date_list.append(date_val.tm_mday)\n",
    "    time_in_min = date_val.tm_hour * 60.0 + date_val.tm_min\n",
    "    date_list.append(time_in_min)\n",
    "    return date_list\n",
    "\n",
    "print train_data[0][0]\n",
    "print convert_date(train_data[0][0])\n",
    "\n",
    "def find_mean_std(train_data, index):\n",
    "    data = []\n",
    "    for row in train_data:\n",
    "        data.append(float(row[index]))\n",
    "    \n",
    "    data_arr = np.array(data, dtype=np.float32)\n",
    "    print np.mean(data_arr)\n",
    "    data_dict = {}\n",
    "    data_dict['mean'] = np.mean(data_arr)\n",
    "    data_dict['std'] = np.std(data_arr)\n",
    "    return data_dict"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Initialized Arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "districts = ['NORTHERN', 'PARK', 'INGLESIDE', 'BAYVIEW', 'RICHMOND', 'CENTRAL', 'TARAVAL', 'TENDERLOIN', 'MISSION', 'SOUTHERN']\n",
    "day = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Engineering - Section I:\n",
    "\n",
    "1) Crime Date - Normalization\n",
    "\n",
    "2) Longitude & Latitude - Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-122.423\n",
      "37.7675\n"
     ]
    }
   ],
   "source": [
    "''' Crime Date - Normalization '''\n",
    "\n",
    "year_data = []\n",
    "mon_data = []\n",
    "time_data = []\n",
    "for row in train_data:\n",
    "    date_list = convert_date(row[0])\n",
    "    year_data.append(float(date_list[0]))\n",
    "    mon_data.append(float(date_list[0]))\n",
    "    time_data.append(date_list[0])\n",
    "\n",
    "year_arr = np.array(year_data, dtype=np.float32)\n",
    "mon_arr = np.array(mon_data, dtype=np.float32)\n",
    "time_arr = np.array(time_data, dtype=np.float32)\n",
    "\n",
    "date_dict = {}\n",
    "date_dict['mean_year'] = np.mean(year_arr)\n",
    "date_dict['std_year'] = np.std(year_arr)                               \n",
    "date_dict['mean_mon'] = np.mean(mon_arr)\n",
    "date_dict['std_mon'] = np.std(mon_arr)\n",
    "date_dict['mean_time'] = np.mean(time_arr)\n",
    "date_dict['std_time'] = np.std(time_arr)\n",
    "\n",
    "''' Longitude & Latitude - Normalization '''\n",
    "long_dict = find_mean_std(train_data, 6)\n",
    "lat_dict = find_mean_std(train_data, 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  2.01500000e+03   5.00000000e+00   1.30000000e+01   1.43300000e+03\n",
      "    0.00000000e+00   0.00000000e+00   1.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   1.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   1.29296839e-01   2.97121316e-01   0.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def get_formatted_data(train_data):\n",
    "    format_data = []\n",
    "    count = 0\n",
    "    \n",
    "    for row in train_data:\n",
    "        count += 1\n",
    "        data = []\n",
    "        \n",
    "        ''' Data Preparation - Crime date '''\n",
    "        \n",
    "        #date_list = convert_date(row[0])\n",
    "        \n",
    "        data.extend(convert_date(row[0])) # Might require improvement\n",
    "        \n",
    "        #year_val = (float(date_list[0]) - date_dict['mean_year'])/date_dict['std_year']\n",
    "        #data.append(year_val)\n",
    "        #mon_val = (float(date_list[1]) - date_dict['mean_mon'])/date_dict['std_mon']\n",
    "        #data.append(mon_val)\n",
    "        #time_val = (float(date_list[2]) - date_dict['mean_time'])/date_dict['std_time']\n",
    "        #data.append(time_val)\n",
    "        \n",
    "        ''' Data Preparation - Day of the Week '''\n",
    "        week_day = row[2]\n",
    "        crime_day = [1 if week_day == d else 0 for d in day] \n",
    "        data.extend(crime_day) # Normalized\n",
    "        \n",
    "        ''' Data Preparation - District '''\n",
    "        pdd = row[3]\n",
    "        pddistrict = [1 if pdd == d else 0 for d in districts] # Normalized\n",
    "        data.extend(pddistrict)\n",
    "        \n",
    "        ''' Data Preparation - Longitude & Latitude '''\n",
    "        longitude = float(row[6])\n",
    "        long_norm = (abs(longitude) - abs(long_dict['mean']))/long_dict['std'] # Normalized\n",
    "        latitude = float(row[7])\n",
    "        lat_norm = (abs(latitude) - abs(lat_dict['mean']))/lat_dict['std'] # Normalized\n",
    "        data.append(long_norm)\n",
    "        data.append(lat_norm)\n",
    "        \n",
    "        ''' Data Preparation - Address '''\n",
    "        \n",
    "        address = row[5].lower()\n",
    "        addr = [1 if 'block' in address else 0] # Requires major improvement\n",
    "        data.extend(addr)\n",
    "        \n",
    "        # Explore external map source\n",
    "        \n",
    "        ''' Quantify the data '''\n",
    "        format_data.append(np.array(data, dtype=np.float32))           \n",
    "        \n",
    "    return format_data\n",
    "    \n",
    "    \n",
    "crimeX = np.array(get_formatted_data(crime_data), dtype=np.float32)\n",
    "trainX = np.array(get_formatted_data(train_data), dtype=np.float32)\n",
    "testX = np.array(get_formatted_data(test_data), dtype=np.float32)\n",
    "devX = np.array(get_formatted_data(dev_data), dtype=np.float32)\n",
    "mini_trainX = np.array(get_formatted_data(mini_train_data), dtype=np.float32)\n",
    "print trainX[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CrimeX Data shape:  (878049L, 24L)\n",
      "CrimeX Label shape:  (878049L, 1L)\n",
      "testX Data shape:  (189025L, 24L)\n",
      "testX Label shape: (189025L, 1L)\n",
      "Mini TrainX Data shape:  (100000L, 24L)\n",
      "Mini TrainX Label shape:  (100000L, 1L)\n"
     ]
    }
   ],
   "source": [
    "print \"CrimeX Data shape: \", crimeX.shape\n",
    "print \"CrimeX Label shape: \", np.array(crime_labels).shape\n",
    "print \"testX Data shape: \", testX.shape\n",
    "print \"testX Label shape:\", np.array(test_labels).shape\n",
    "print \"Mini TrainX Data shape: \", mini_trainX.shape\n",
    "print \"Mini TrainX Label shape: \", np.array(mini_train_labels).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_labels = np.array(train_labels).ravel()\n",
    "test_labels = np.array(test_labels).ravel()\n",
    "crime_labels = np.array(crime_labels).ravel()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Building the Logistic Regression model on the entire Crime Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed the training\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression()\n",
    "lr.fit(crimeX, crime_labels)\n",
    "print \"Completed the training\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.314998194696\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\metrics\\metrics.py:1773: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "mini_testX, mini_test_labels = testX[:10000], test_labels[:10000]\n",
    "f1_score = metrics.f1_score(lr.predict(mini_testX), mini_test_labels)\n",
    "print f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "print \"hello\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sub_test_data = []\n",
    "with open(\"../test.csv\", \"rb\") as csvfile:\n",
    "    reader = csv.reader(csvfile, delimiter=',')\n",
    "    first_row = True\n",
    "    for row in reader:\n",
    "        data = [y for x, y in enumerate(row)]\n",
    "        if first_row:\n",
    "            first_row = False\n",
    "        else:\n",
    "            sub_test_data.append(data)\n",
    "            \n",
    "\n",
    "def get_formatted_test_data(test_data):\n",
    "    format_data = []\n",
    "    count = 0\n",
    "    \n",
    "    for row in test_data:\n",
    "        count += 1\n",
    "        data = []\n",
    "        data.extend(convert_date(row[1]))\n",
    "        week_day = row[2]\n",
    "        crime_day = [1 if week_day == d else 0 for d in day] \n",
    "        data.extend(crime_day)\n",
    "        pdd = row[3]\n",
    "        pddistrict = [1 if pdd == d else 0 for d in districts]\n",
    "        data.extend(pddistrict)\n",
    "        longitude = float(row[5])\n",
    "        long_norm = (abs(longitude) - abs(long_dict['mean']))/long_dict['std']\n",
    "        latitude = float(row[6])\n",
    "        lat_norm = (abs(latitude) - abs(lat_dict['mean']))/lat_dict['std']\n",
    "        data.append(long_norm)\n",
    "        data.append(lat_norm)\n",
    "        \n",
    "        if 'block' in row[4].lower():\n",
    "            address = 1\n",
    "        else:\n",
    "            address = 0\n",
    "        data.append(address)\n",
    "        format_data.append(np.array(data, dtype=np.float32))           \n",
    "        \n",
    "    return format_data\n",
    "    \n",
    "    \n",
    "\n",
    "testX = np.array(get_formatted_test_data(sub_test_data), dtype=np.float32)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(884262L, 39L)\n",
      "[[  3.19594531e-03   1.11893378e-01   9.99398335e-05   6.87935482e-04\n",
      "    4.84346270e-02   1.60246933e-03   8.95964469e-04   4.53638414e-02\n",
      "    4.03224385e-03   4.32879921e-04   1.37376523e-04   8.49872448e-04\n",
      "    4.40179386e-03   8.40223308e-03   2.44692320e-04   3.79233163e-03\n",
      "    1.51675058e-01   1.70899243e-03   1.82237828e-04   6.56120507e-02\n",
      "    8.98320326e-02   1.22974919e-01   1.66476422e-05   2.12681594e-04\n",
      "    4.86397578e-03   2.04967091e-02   4.48285127e-03   2.00269473e-02\n",
      "    3.74452528e-03   1.79955227e-04   3.96028544e-03   4.46248133e-04\n",
      "    3.48269309e-02   8.51272773e-06   6.29219886e-03   6.90577879e-02\n",
      "    1.00957971e-01   4.42163318e-02   1.97566245e-02]]\n"
     ]
    }
   ],
   "source": [
    "print probs.shape\n",
    "print probs[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "probs = lr.predict_proba(testX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job completed\n"
     ]
    }
   ],
   "source": [
    "import gzip\n",
    "with gzip.open('submission-matrix.csv.gz', 'wb') as f:\n",
    "    out = csv.writer(f, lineterminator='\\n')\n",
    "    out.writerow(['Id'] + list(np.unique(crime_labels)))\n",
    "    \n",
    "    for i, prob in enumerate(probs):\n",
    "        out.writerow([i] + list(prob))\n",
    "print \"Job completed\"     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
