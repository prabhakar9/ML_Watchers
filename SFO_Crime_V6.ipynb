{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# General Libraries\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "# SK-learn libraries for learning.\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.mixture import GMM\n",
    "from matplotlib.colors import LogNorm\n",
    "\n",
    "# SK-learn libraries for evaluation.\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "# SK-learn libraries for feature extraction from text.\n",
    "from sklearn.feature_extraction.text import *\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "# Keras\n",
    "from keras.layers.advanced_activations import PReLU\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.models import Sequential\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the Crime data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Dates', 'Descript', 'DayOfWeek', 'PdDistrict', 'Resolution', 'Address', 'X', 'Y']\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'crimea_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-103-7a291815dc2e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     17\u001b[0m             \u001b[0mcrime_alabels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m \u001b[1;32mprint\u001b[0m \u001b[0mcrimea_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'crimea_data' is not defined"
     ]
    }
   ],
   "source": [
    "crime_adata = []\n",
    "crime_alabels = []\n",
    "with open(\"../train.csv\", \"rb\") as csvfile:\n",
    "    reader = csv.reader(csvfile, delimiter=',')\n",
    "    first_row = True\n",
    "    for row in reader:\n",
    "        data = [y for x, y in enumerate(row) if x != 1]\n",
    "        name = [y for x, y in enumerate(row) if x == 1]\n",
    "        \n",
    "        if first_row:\n",
    "            first_row = False\n",
    "            feature_names = data\n",
    "            print feature_names\n",
    "        else:\n",
    "            crime_adata.append(data)\n",
    "            \n",
    "            crime_alabels.append(name)\n",
    "\n",
    "print crimea_data[:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Divide the Crime data into Train data, test data, mini train data, and dev sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Divide the Crime training set into train data, test data, dev data\n",
    "train_data, train_labels = crimea_adata[:500000], crime_alabels[:500000]\n",
    "crime_test, crime_test_labels = crime_adata[500000:], crime_alabels[500000:]\n",
    "num_test = len(crime_test)\n",
    "print num_test\n",
    "dev_data, dev_labels = crime_test[:num_test/2], crime_test_labels[:num_test/2]\n",
    "test_data, test_labels = crime_test[num_test/2:], crime_test_labels[num_test/2:]\n",
    "mini_train_data, mini_train_labels = crime_adata[200000:300000], crime_alabels[200000:300000]\n",
    "\n",
    "print \"Size of train data: \", len(train_data)\n",
    "print \"Size of dev data: \", len(dev_data)\n",
    "print \"Size of test data: \", len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_df = pd.read_csv(\"../train.csv\")\n",
    "data_df_orig = data_df.copy()\n",
    "data_df = data_df[data_df.Y != 90]\n",
    "crime_data = np.array(data_df[['Dates', 'Descript', 'DayOfWeek', 'PdDistrict', 'Resolution', 'Address', 'X', 'Y']].values)\n",
    "crime_labels = np.array(data_df[['Category']].values.ravel())\n",
    "print crime_data[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0L '2015-05-10 23:59:00' 'Sunday' 'BAYVIEW' '2000 Block of THOMAS AV'\n",
      "  -122.39958770418998 37.7350510103906]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['Id', 'Dates', 'DayOfWeek', 'PdDistrict', 'Address', 'X', 'Y'], dtype=object)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data_df = pd.read_csv(\"../test.csv\")\n",
    "test_data = np.array(test_data_df.values)\n",
    "print test_data[:1]\n",
    "test_data_df.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Creating the clusters for X, Y Coordinates\n",
    "data_xy = np.array(data_df_orig[['X', 'Y']].values)\n",
    "km = KMeans(n_clusters=100)\n",
    "X_fit = km.fit(data_xy)\n",
    "y = km.labels_\n",
    "clusters = X_fit.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_xy = np.array(test_data_df[['X', 'Y']].values)\n",
    "test_clusters = km.predict(test_xy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next section has all UTIL functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[33 33 42 89 20 31 22 15 56 81 81 28 27 88  4 73 27 55 73 27]\n",
      "[ 4 30 13 64 64 60 55 44 83 97 21 69 69 18 42 75 37 31 24 13]\n"
     ]
    }
   ],
   "source": [
    "clusters.shape\n",
    "print clusters[:20]\n",
    "test_clusters.shape\n",
    "print test_clusters[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2015-05-13 23:53:00\n",
      "[2015, 5, 13, 1433.0, 23, 'Night']\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "def convert_date(date_val):\n",
    "    date_list = []\n",
    "    date, time = date_val.split()\n",
    "    date_list.append(int(date.split('-')[0]))\n",
    "    date_list.append(int(date.split('-')[1]))\n",
    "    date_list.append(int(date.split('-')[2]))\n",
    "    time_in_min = int(time.split(':')[0]) * 60.0 + int(time.split(':')[1])\n",
    "    date_list.append(time_in_min)\n",
    "    date_list.append(int(time.split(':')[0]))\n",
    "    time_hour = int(time.split(':')[0])\n",
    "    if time_hour < 6:\n",
    "        time_of_day = 'Twilight'\n",
    "    elif time_hour < 12:\n",
    "        time_of_day = 'Morning'\n",
    "    elif time_hour < 18:\n",
    "        time_of_day = 'Afternoon'\n",
    "    else:\n",
    "        time_of_day = 'Night'\n",
    "    date_list.append(time_of_day)\n",
    "    return date_list\n",
    "\n",
    "print train_data[0][0]\n",
    "print convert_date(train_data[0][0])\n",
    "\n",
    "def find_mean_std(train_data, index):\n",
    "    data = []\n",
    "    for row in train_data:\n",
    "        data.append(float(row[index]))\n",
    "    \n",
    "    data_arr = np.array(data, dtype=np.float32)\n",
    "    print np.mean(data_arr)\n",
    "    data_dict = {}\n",
    "    data_dict['mean'] = np.mean(data_arr)\n",
    "    data_dict['std'] = np.std(data_arr)\n",
    "    return data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "date_arr = convert_date(train_data[0][0])\n",
    "year, month, day, time_in_min, hour, time_of_day = date_arr\n",
    "\n",
    "cyear = [(year < 2006) * 1, (year < 2008) * 1, (year < 2010) * 1, (year < 2012)*1, (year < 2015)*1, (year == 2015)*1]\n",
    "print cyear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialized Arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "districts = ['NORTHERN', 'PARK', 'INGLESIDE', 'BAYVIEW', 'RICHMOND', 'CENTRAL', 'TARAVAL', 'TENDERLOIN', 'MISSION', 'SOUTHERN']\n",
    "week_day = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "daytime = ['Twilight', 'Morning', 'Afternoon', 'Night']\n",
    "year_range = ['2003-2006', '2006-2009', '2009-2012', 'After 2015']\n",
    "crime_year = [2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015]\n",
    "crime_month = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
    "crime_day = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29 ,30 ,31]\n",
    "crime_hour = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Engineering - Section I:\n",
    "\n",
    "1) Crime Date - Normalization\n",
    "\n",
    "2) Longitude & Latitude - Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-122.423\n",
      "37.7675\n"
     ]
    }
   ],
   "source": [
    "''' Crime Date - Normalization '''\n",
    "\n",
    "year_data = []\n",
    "mon_data = []\n",
    "day_data = []\n",
    "time_data = []\n",
    "time_of_day_data = []\n",
    "for row in crime_data:\n",
    "    date_arr = convert_date(row[0])\n",
    "    year_data.append(float(date_arr[0]))\n",
    "    mon_data.append(float(date_arr[1]))\n",
    "    day_data.append(float(date_arr[2]))\n",
    "    time_data.append(float(date_arr[3]))\n",
    "    time_of_day_data.append(date_arr[5])\n",
    "                     \n",
    "\n",
    "year_arr = np.array(year_data, dtype=np.float32)\n",
    "mon_arr = np.array(mon_data, dtype=np.float32)\n",
    "day_arr = np.array(day_data, dtype=np.float32)\n",
    "time_arr = np.array(time_data, dtype=np.float32)\n",
    "time_of_day_arr = np.array(time_of_day_data)\n",
    "\n",
    "date_dict = {}\n",
    "date_dict['mean_year'] = np.mean(year_arr)\n",
    "date_dict['std_year'] = np.std(year_arr)                               \n",
    "date_dict['mean_mon'] = np.mean(mon_arr)\n",
    "date_dict['std_mon'] = np.std(mon_arr)\n",
    "date_dict['mean_day'] = np.mean(day_arr)\n",
    "date_dict['std_day'] = np.std(day_arr)\n",
    "date_dict['mean_time'] = np.mean(time_arr)\n",
    "date_dict['std_time'] = np.std(time_arr)\n",
    "\n",
    "\n",
    "''' Longitude & Latitude - Normalization '''\n",
    "long_dict = find_mean_std(train_data, 6)\n",
    "lat_dict = find_mean_std(train_data, 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0.           0.           0.           0.           0.           0.\n",
      "    0.           0.           0.           0.           0.           0.\n",
      "    1.           0.           0.           0.           0.           1.\n",
      "    0.           0.           0.           0.           0.           0.\n",
      "    0.           0.           0.           0.           0.           0.\n",
      "    0.           0.           0.           0.           0.           0.\n",
      "    0.           1.           0.           0.           0.           0.\n",
      "    0.           0.           0.           0.           0.           0.\n",
      "    0.           0.           0.           0.           0.           0.\n",
      "    0.           0.           0.           0.           0.           0.\n",
      "    0.           0.           0.           0.           0.           0.\n",
      "    0.           0.           0.           0.           0.           0.\n",
      "    0.           0.           0.           0.           0.           0.\n",
      "    0.           1.           0.           0.           0.           1.\n",
      "    0.           0.           1.           0.           0.           0.\n",
      "    0.           1.           0.           0.           0.           0.\n",
      "    0.           0.           0.           0.           0.           0.12929684\n",
      "    0.29712132  33.           0.        ]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def get_formatted_data(train_data):\n",
    "    format_data = []\n",
    "    count = 0\n",
    "    \n",
    "    for i in range(len(train_data)):\n",
    "        count += 1\n",
    "        data = []\n",
    "        row = train_data[i]\n",
    "        \n",
    "        ''' Data Preparation - Crime date '''\n",
    "        \n",
    "        #date_list = convert_date(row[0])\n",
    "        # [2015, 5, 13, 1433.0, 23, 'Night']\n",
    "        \n",
    "        date_arr = convert_date(row[0])\n",
    "        year, month, day, time_in_min, hour, time_of_day = date_arr\n",
    "        \n",
    "        norm_year = [1 if year == y else 0 for y in crime_year]\n",
    "        data.extend(norm_year)\n",
    "        \n",
    "        norm_month = [1 if month == m else 0 for m in crime_month]\n",
    "        data.extend(norm_month)\n",
    "        \n",
    "        norm_day = [1 if day == d else 0 for d in crime_day]\n",
    "        data.extend(norm_day)\n",
    "        \n",
    "        norm_hour = [1 if hour == h else 0 for h in crime_hour]\n",
    "        data.extend(norm_hour)\n",
    "        \n",
    "        # Time of the day preparation\n",
    "        day_time = [1 if time_of_day == td else 0 for td in daytime]\n",
    "        data.extend(day_time)\n",
    "        \n",
    "        \n",
    "        ''' Data Preparation - Day of the Week '''\n",
    "        wk_day = row[2]\n",
    "        \n",
    "        crime_week_day = [1 if wk_day == d else 0 for d in week_day] \n",
    "        data.extend(crime_week_day) # Normalized\n",
    "        \n",
    "        ''' Data Preparation - District '''\n",
    "        pdd = row[3]\n",
    "        pddistrict = [1 if pdd == d else 0 for d in districts] # Normalized\n",
    "        data.extend(pddistrict)\n",
    "        \n",
    "        ''' Data Preparation - Longitude & Latitude '''\n",
    "        longitude = float(row[6])\n",
    "        long_norm = (abs(longitude) - abs(long_dict['mean']))/long_dict['std'] # Normalized\n",
    "        latitude = float(row[7])\n",
    "        lat_norm = (abs(latitude) - abs(lat_dict['mean']))/lat_dict['std'] # Normalized\n",
    "        \n",
    "        data.append(long_norm)\n",
    "        data.append(lat_norm)\n",
    "        \n",
    "        cluster_label = clusters[i]\n",
    "        data.append(cluster_label)\n",
    "        \n",
    "        ''' Data Preparation - Address '''\n",
    "        \n",
    "        address = row[5].lower()\n",
    "        addr = [1 if 'block' in address else 0] # Requires major improvement\n",
    "        data.extend(addr)\n",
    "        \n",
    "        # Explore external map source\n",
    "        \n",
    "        ''' Quantify the data '''\n",
    "        format_data.append(np.array(data, dtype=np.float32))           \n",
    "        \n",
    "    return format_data\n",
    "    \n",
    "    \n",
    "crimeX = np.array(get_formatted_data(crime_data), dtype=np.float32)\n",
    "print crimeX[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(877982L, 105L)\n"
     ]
    }
   ],
   "source": [
    "print crimeX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CrimeX Data shape:  (877982L, 105L)\n",
      "CrimeX Label shape:  (877982L,)\n"
     ]
    }
   ],
   "source": [
    "print \"CrimeX Data shape: \", crimeX.shape\n",
    "print \"CrimeX Label shape: \", np.array(crime_labels).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building the Logistic Regression model on the entire Crime Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 0.01}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pgundugola\\Downloads\\WinPython-64bit-2.7.10.2\\python-2.7.10.amd64\\lib\\site-packages\\sklearn\\cross_validation.py:417: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of labels for any class cannot be less than n_folds=3.\n",
      "  % (min_labels, self.n_folds)), Warning)\n"
     ]
    }
   ],
   "source": [
    "params = {'C':[0.001, 0.01, 0.1, 0.2, 1, 10, 100]}\n",
    "clf = GridSearchCV(LogisticRegression(penalty='l2'), params)\n",
    "clf.fit(crimeX[:10000], crime_labels[:10000])\n",
    "print clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed training with best C\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(C=0.01)\n",
    "lr.fit(crimeX, crime_labels)\n",
    "print \"Completed training with best C\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "print 'hello'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_formatted_test_data(test_data):\n",
    "    format_data = []\n",
    "    count = 0\n",
    "    \n",
    "    for i in range(len(test_data)):\n",
    "        count += 1\n",
    "        data = []\n",
    "        \n",
    "        row = test_data[i]\n",
    "        \n",
    "        ''' Data Preparation - Date '''\n",
    "        date_arr = convert_date(row[1])\n",
    "        year, month, day, time_in_min, hour, time_of_day = date_arr\n",
    "        \n",
    "        norm_year = [1 if year == y else 0 for y in crime_year]\n",
    "        data.extend(norm_year)\n",
    "        \n",
    "        norm_month = [1 if month == m else 0 for m in crime_month]\n",
    "        data.extend(norm_month)\n",
    "        \n",
    "        norm_day = [1 if day == d else 0 for d in crime_day]\n",
    "        data.extend(norm_day)\n",
    "        \n",
    "        norm_hour = [1 if hour == h else 0 for h in crime_hour]\n",
    "        data.extend(norm_hour)\n",
    "        \n",
    "        # Time of the day preparation\n",
    "        day_time = [1 if time_of_day == td else 0 for td in daytime]\n",
    "        data.extend(day_time)\n",
    "        \n",
    "        ''' Data Preparation - Day of the Week '''\n",
    "        wk_day = row[2]\n",
    "        \n",
    "        crime_week_day = [1 if wk_day == d else 0 for d in week_day] \n",
    "        data.extend(crime_week_day) # Normalized\n",
    "        \n",
    "        pdd = row[3]\n",
    "        pddistrict = [1 if pdd == d else 0 for d in districts]\n",
    "        data.extend(pddistrict)\n",
    "        \n",
    "        longitude = float(row[5])\n",
    "        long_norm = (abs(longitude) - abs(long_dict['mean']))/long_dict['std']\n",
    "        latitude = float(row[6])\n",
    "        lat_norm = (abs(latitude) - abs(lat_dict['mean']))/lat_dict['std']\n",
    "        data.append(long_norm)\n",
    "        data.append(lat_norm)\n",
    "        \n",
    "        test_cluster_label = test_clusters[i]\n",
    "        data.append(test_cluster_label)\n",
    "        \n",
    "        if 'block' in row[4].lower():\n",
    "            address = 1\n",
    "        else:\n",
    "            address = 0\n",
    "        data.append(address)\n",
    "        format_data.append(np.array(data, dtype=np.float32))           \n",
    "        \n",
    "    return format_data\n",
    "    \n",
    "    \n",
    "\n",
    "testX = np.array(get_formatted_test_data(test_data), dtype=np.float32)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(884262L, 105L)\n"
     ]
    }
   ],
   "source": [
    "print testX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(884262L, 39L)\n",
      "[[ 0.0081226   0.13772509  0.00088868  0.00290412  0.03885434  0.00221439\n",
      "   0.00380181  0.02013473  0.00632898  0.00099729  0.00131804  0.00189373\n",
      "   0.00184039  0.00684142  0.00162343  0.00663832  0.14582495  0.00251005\n",
      "   0.00046194  0.05445176  0.0835123   0.11219629  0.00144325  0.0002684\n",
      "   0.00363221  0.03229263  0.00371116  0.03346342  0.00812909  0.00173907\n",
      "   0.00522259  0.00147863  0.03644636  0.00064517  0.00632584  0.08039471\n",
      "   0.08459471  0.03469086  0.02443723]]\n"
     ]
    }
   ],
   "source": [
    "print probs.shape\n",
    "print probs[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(884262L, 39L)\n",
      "[[ 0.00598014  0.14235451  0.00039644  0.00095056  0.04091489  0.00200755\n",
      "   0.00284076  0.01751568  0.0054245   0.00065421  0.00042781  0.00088631\n",
      "   0.00179967  0.00680445  0.00042315  0.00505384  0.15562875  0.00179362\n",
      "   0.00036821  0.05546664  0.08141095  0.11349672  0.00025786  0.00034429\n",
      "   0.00303172  0.03292385  0.00276192  0.0313208   0.00717204  0.00044138\n",
      "   0.00499776  0.0007008   0.03670595  0.00025392  0.00626717  0.08456407\n",
      "   0.08833596  0.03474202  0.02257913]]\n"
     ]
    }
   ],
   "source": [
    "print probs.shape\n",
    "print probs[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "probs = lr.predict_proba(testX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job completed\n"
     ]
    }
   ],
   "source": [
    "import gzip\n",
    "with gzip.open('submission-matrix-3.csv.gz', 'wb') as f:\n",
    "    out = csv.writer(f, lineterminator='\\n')\n",
    "    out.writerow(['Id'] + list(np.unique(crime_labels)))\n",
    "    \n",
    "    for i, prob in enumerate(probs):\n",
    "        out.writerow([i] + list(prob))\n",
    "print \"Job completed\"     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Boosting Completed\n"
     ]
    }
   ],
   "source": [
    "gclf = GradientBoostingClassifier()\n",
    "gclf.fit(crimeX, crime_labels)\n",
    "print \"Gradient Boosting Completed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "print \"hello\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job completed\n"
     ]
    }
   ],
   "source": [
    "gclf_probs = gclf.predict_proba(testX)\n",
    "import gzip\n",
    "with gzip.open('submission-matrix-gclf.csv.gz', 'wb') as f:\n",
    "    out = csv.writer(f, lineterminator='\\n')\n",
    "    out.writerow(['Id'] + list(np.unique(crime_labels)))\n",
    "    \n",
    "    for i, prob in enumerate(gclf_probs):\n",
    "        out.writerow([i] + list(prob))\n",
    "print \"Job completed\"   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(884262L, 39L)\n",
      "[[  3.02440836e-03   1.43862674e-01   2.03561777e-04   1.08661515e-03\n",
      "    4.46352994e-02   1.79431247e-03   4.26209288e-03   2.22077394e-02\n",
      "    4.31961594e-03   8.31534882e-04   2.49473397e-04   1.01547540e-03\n",
      "    3.65270688e-03   8.62011288e-03   1.90901827e-04   4.25244762e-03\n",
      "    1.29711833e-01   1.30613090e-03   3.64729688e-04   6.75900961e-02\n",
      "    7.56290451e-02   1.03543899e-01   1.68236500e-05   5.60963927e-04\n",
      "    1.56161420e-03   3.96501576e-02   4.48375028e-03   2.18597194e-02\n",
      "    5.38789645e-03   2.49638507e-04   4.96575344e-03   4.87584338e-04\n",
      "    4.72745625e-02   3.67647530e-06   6.41916468e-03   9.32536637e-02\n",
      "    9.39834953e-02   3.89940423e-02   1.84927866e-02]]\n"
     ]
    }
   ],
   "source": [
    "print gclf_probs.shape\n",
    "print gclf_probs[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Training Completed\n"
     ]
    }
   ],
   "source": [
    "clf = RandomForestClassifier(n_estimators=25)\n",
    "clf.fit(crimeX, crime_labels)\n",
    "print \"Random Forest Training Completed\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.882141746499\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\metrics\\classification.py:676: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=\"f1_weighted\" instead of scoring=\"f1\".\n",
      "  sample_weight=sample_weight)\n"
     ]
    }
   ],
   "source": [
    "mini_testX, mini_test_labels = testX[:10000], test_labels[:10000]\n",
    "f1_score = metrics.f1_score(clf.predict(mini_testX), mini_test_labels)\n",
    "print f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(884262L, 39L)\n",
      "[[ 0.          0.18        0.          0.          0.01333333  0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.48        0.          0.          0.03        0.07\n",
      "   0.02        0.          0.          0.          0.02        0.          0.\n",
      "   0.          0.          0.          0.          0.04        0.          0.\n",
      "   0.04        0.08        0.02666667  0.        ]]\n"
     ]
    }
   ],
   "source": [
    "clf_probs = clf.predict_proba(testX)\n",
    "print clf_probs.shape\n",
    "print clf_probs[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job completed\n"
     ]
    }
   ],
   "source": [
    "import gzip\n",
    "with gzip.open('submission-matrix.csv.gz', 'wb') as f:\n",
    "    out = csv.writer(f, lineterminator='\\n')\n",
    "    out.writerow(['Id'] + list(np.unique(crime_labels)))\n",
    "    \n",
    "    for i, prob in enumerate(clf_probs):\n",
    "        out.writerow([i] + list(prob))\n",
    "print \"Job completed\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_xy = np.array(data_df_orig[['X', 'Y']].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "km = KMeans(n_clusters=20)\n",
    "X_fit = km.fit(data_xy)\n",
    "y = km.labels_\n",
    "clusters = X_fit.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(878049L,)\n",
      "(878049L,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([13, 13, 27, 27, 52])"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print y.shape\n",
    "print clusters.shape\n",
    "clusters[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x89d16ef0>"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAEACAYAAACznAEdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x3ac0ff98>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(data_xy[:,0], data_xy[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pgundugola\\Downloads\\WinPython-64bit-2.7.10.2\\python-2.7.10.amd64\\lib\\site-packages\\sklearn\\metrics\\classification.py:676: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=\"f1_weighted\" instead of scoring=\"f1\".\n",
      "  sample_weight=sample_weight)\n",
      "C:\\Users\\pgundugola\\Downloads\\WinPython-64bit-2.7.10.2\\python-2.7.10.amd64\\lib\\site-packages\\sklearn\\metrics\\classification.py:958: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BernoulliNB Results\n",
      "--------------------\n",
      "Accuracy:  0.08413\n",
      "F1 Score:  0.0797225803152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pgundugola\\Downloads\\WinPython-64bit-2.7.10.2\\python-2.7.10.amd64\\lib\\site-packages\\sklearn\\metrics\\classification.py:960: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "mini_testX, mini_test_labels = crimeX[800000:], crime_labels[800000:]\n",
    "mini_devX, mini_dev_labels = crimeX[700000:800000], crime_labels[700000:800000]\n",
    "mini_trainX, mini_train_labels = crimeX[:200000], crime_labels[:200000]\n",
    "bnbb = BernoulliNB()\n",
    "bnbb.fit(mini_trainX, mini_train_labels)\n",
    "bnbb_probs = bnbb.predict_proba(mini_testX)\n",
    "bnbb_accuracy = bnbb.score(mini_devX, mini_dev_labels)\n",
    "# bnbb_log_loss = log_loss(mini_test_labels, bnbb_probs)\n",
    "f1_score = metrics.f1_score(bnbb.predict(mini_testX), mini_test_labels)\n",
    "\n",
    "\n",
    "print \"BernoulliNB Results\"\n",
    "print \"--------------------\"\n",
    "print \"Accuracy: \", bnbb_accuracy\n",
    "print \"F1 Score: \", f1_score\n",
    "#print \"Log Loss: \", bnbb_log_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Results\n",
      "--------------------\n",
      "Accuracy:  0.18898\n",
      "F1 Score:  0.272224895686\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pgundugola\\Downloads\\WinPython-64bit-2.7.10.2\\python-2.7.10.amd64\\lib\\site-packages\\sklearn\\metrics\\classification.py:676: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=\"f1_weighted\" instead of scoring=\"f1\".\n",
      "  sample_weight=sample_weight)\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(C=0.01)\n",
    "lr.fit(mini_trainX, mini_train_labels)\n",
    "lr_probs = lr.predict_proba(mini_testX)\n",
    "lr_accuracy = lr.score(mini_devX, mini_dev_labels)\n",
    "#lr_log_loss = log_loss(mini_test_labels, lr.predict_proba(mini_testX))\n",
    "f1_score = metrics.f1_score(lr.predict(mini_testX), mini_test_labels)\n",
    "\n",
    "print \"Logistic Regression Results\"\n",
    "print \"--------------------\"\n",
    "print \"Accuracy: \", lr_accuracy\n",
    "#print \"Log Loss: \", lr_log_loss\n",
    "print \"F1 Score: \", f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Regression Results\n",
      "--------------------\n",
      "Accuracy:  0.19042\n",
      "F1 Score:  0.261775567148\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pgundugola\\Downloads\\WinPython-64bit-2.7.10.2\\python-2.7.10.amd64\\lib\\site-packages\\sklearn\\metrics\\classification.py:676: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=\"f1_weighted\" instead of scoring=\"f1\".\n",
      "  sample_weight=sample_weight)\n"
     ]
    }
   ],
   "source": [
    "rfclf = RandomForestClassifier(n_estimators=50)\n",
    "rfclf.fit(mini_trainX, mini_train_labels)\n",
    "rfclf_probs = rfclf.predict_proba(mini_testX)\n",
    "rfclf_accuracy = rfclf.score(mini_devX, mini_dev_labels)\n",
    "#lr_log_loss = log_loss(mini_test_labels, lr.predict_proba(mini_testX))\n",
    "f1_score = metrics.f1_score(rfclf.predict(mini_testX), mini_test_labels)\n",
    "\n",
    "print \"Random Forest Regression Results\"\n",
    "print \"--------------------\"\n",
    "print \"Accuracy: \", rfclf_accuracy\n",
    "#print \"Log Loss: \", lr_log_loss\n",
    "print \"F1 Score: \", f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "label_fields = {'Category': lambda x: labels.index(x.replace(',', ''))}\n",
    "\n",
    "def get_fields(data, fields):\n",
    "  extracted = []\n",
    "  for row in data:\n",
    "    extract = []\n",
    "    for field, f in sorted(fields.items()):\n",
    "      info = f(row[field])\n",
    "      if type(info) == list:\n",
    "        extract.extend(info)\n",
    "      else:\n",
    "        extract.append(info)\n",
    "    extracted.append(np.array(extract, dtype=np.float32))\n",
    "  return extracted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data preprocessed\n"
     ]
    }
   ],
   "source": [
    "labels = 'ARSON,ASSAULT,BAD CHECKS,BRIBERY,BURGLARY,DISORDERLY CONDUCT,DRIVING UNDER THE INFLUENCE,DRUG/NARCOTIC,DRUNKENNESS,EMBEZZLEMENT,EXTORTION,FAMILY OFFENSES,FORGERY/COUNTERFEITING,FRAUD,GAMBLING,KIDNAPPING,LARCENY/THEFT,LIQUOR LAWS,LOITERING,MISSING PERSON,NON-CRIMINAL,OTHER OFFENSES,PORNOGRAPHY/OBSCENE MAT,PROSTITUTION,RECOVERED VEHICLE,ROBBERY,RUNAWAY,SECONDARY CODES,SEX OFFENSES FORCIBLE,SEX OFFENSES NON FORCIBLE,STOLEN PROPERTY,SUICIDE,SUSPICIOUS OCC,TREA,TRESPASS,VANDALISM,VEHICLE THEFT,WARRANTS,WEAPON LAWS'.split(',')\n",
    "label_fields = {'Category': lambda x: labels.index(x.replace(',', ''))}\n",
    "crime_year = [2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015]\n",
    "crime_month = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
    "crime_day = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29 ,30 ,31]\n",
    "crime_hour = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]\n",
    "\n",
    "def dating(x):\n",
    "    data = []\n",
    "    date, time = x.split(' ')\n",
    "    y, m, d = map(int, date.split('-'))\n",
    "    time = time.split(':')[:2]\n",
    "    time1 = time\n",
    "    time = int(time[0]) * 60 + int(time[1])\n",
    "    year = [1 if y == yr else 0 for yr in crime_year]\n",
    "    data.extend(year)\n",
    "    month = [1 if m == mon else 0 for mon in crime_month]\n",
    "    data.extend(month)\n",
    "    day = [1 if d == dy else 0 for dy in crime_day]\n",
    "    data.extend(day)\n",
    "    h = int(time1[0])\n",
    "    hour = [1 if h == hr else 0 for hr in crime_hour]\n",
    "    data.extend(hour)\n",
    "    return data\n",
    "\n",
    "def norm_long(x):\n",
    "    return (abs(x) - abs(long_dict['mean']))/long_dict['std']\n",
    "\n",
    "def norm_lat(x):\n",
    "    return (abs(x) - abs(lat_dict['mean']))/lat_dict['std']\n",
    "\n",
    "data_fields = {\n",
    "    'X': lambda x: float(x),\n",
    "    'Y': lambda x: float(y),\n",
    "    'DayOfWeek': lambda x:[1 if x == wd else 0 for wd in week_day],\n",
    "    'Address': lambda x: [1 if 'block' in x.lower() else 0],\n",
    "    'PdDistrict': lambda x: [1 if x == d else 0 for d in districts],\n",
    "    'Dates': dating,  # Parse 2015-05-13 23:53:00\n",
    "}\n",
    "\n",
    "def get_data(fn):\n",
    "  data = []\n",
    "  with open(fn) as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    data = [row for row in reader]\n",
    "  return data\n",
    "\n",
    "def get_fields(data, fields):\n",
    "  extracted = []\n",
    "  for row in data:\n",
    "    extract = []\n",
    "    for field, f in sorted(fields.items()):\n",
    "      info = f(row[field])\n",
    "      if type(info) == list:\n",
    "        extract.extend(info)\n",
    "      else:\n",
    "        extract.append(info)\n",
    "    extracted.append(np.array(extract, dtype=np.float32))\n",
    "  return extracted\n",
    "\n",
    "def preprocess_data(X, scaler=None):\n",
    "  if not scaler:\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X)\n",
    "  X = scaler.transform(X)\n",
    "  return X, scaler\n",
    "\n",
    "def shuffle(X, y, seed=1337):\n",
    "  np.random.seed(seed)\n",
    "  shuffle = np.arange(len(y))\n",
    "  np.random.shuffle(shuffle)\n",
    "  X = X[shuffle]\n",
    "  y = y[shuffle]\n",
    "  return X, y\n",
    "\n",
    "\n",
    "raw_train = get_data('../train.csv')\n",
    "X = np.array(get_fields(raw_train, data_fields), dtype=np.float32)\n",
    "y = np.array(get_fields(raw_train, label_fields))\n",
    "X, y = shuffle(X, y)\n",
    "X, scaler = preprocess_data(X)\n",
    "del raw_train\n",
    "Y = np_utils.to_categorical(y)\n",
    "print \"Training data preprocessed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating testing data...\n"
     ]
    }
   ],
   "source": [
    "raw_test = get_data('../test.csv')\n",
    "print('Creating testing data...')\n",
    "X_test = np.array(get_fields(raw_test, data_fields), dtype=np.float32)\n",
    "del raw_test\n",
    "X_test, _ = preprocess_data(X_test, scaler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def get_model(input_dim, output_dim, wbit, dp, layers):\n",
    "    keras = Sequential()\n",
    "    keras.add(Dense(input_dim, wbit, init='glorot_uniform'))\n",
    "    keras.add(PReLU((wbit,)))\n",
    "    keras.add(Dropout(dp))\n",
    "\n",
    "    for i in range(layers):\n",
    "        keras.add(Dense(wbit, wbit, init='glorot_uniform'))\n",
    "        keras.add(PReLU((wbit,)))\n",
    "        keras.add(BatchNormalization((wbit,)))\n",
    "        keras.add(Dropout(dp))\n",
    "\n",
    "    keras.add(Dense(wbit, output_dim, init='glorot_uniform'))\n",
    "    keras.add(Activation('softmax'))\n",
    "    keras.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "    return keras\n",
    "\n",
    "input_dim = X.shape[1]\n",
    "output_dim = 39\n",
    "wbit = 64\n",
    "EPOCHS = 1\n",
    "RUN_FOLDS = False\n",
    "BATCHES = 128\n",
    "nb_folds = 4\n",
    "dp = 0.5\n",
    "layers = 3\n",
    "kfolds = KFold(len(crime_labels), nb_folds)\n",
    "keras = get_model(input_dim, output_dim, wbit, dp, layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2151a1e48>"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.fit(X, Y, nb_epoch=1, batch_size=BATCHES, verbose=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  3.48426832e-03   1.16913358e-01   9.31749807e-04   7.18774146e-04\n",
      "    6.29154016e-02   6.94968199e-03   3.52990018e-03   4.30306703e-02\n",
      "    5.76666126e-03   2.31244903e-03   6.48691587e-04   1.13350282e-03\n",
      "    1.73865042e-02   1.98705418e-02   4.62154143e-04   4.85318963e-03\n",
      "    1.05479662e-01   2.92981061e-03   1.51546359e-03   5.71302423e-02\n",
      "    8.23094002e-02   1.23750257e-01   1.24792768e-04   6.28301369e-03\n",
      "    7.41351862e-03   2.54724525e-02   4.69753348e-03   1.85440104e-02\n",
      "    8.01852512e-03   4.87338186e-04   5.68227027e-03   1.02860750e-03\n",
      "    4.56800496e-02   7.14054225e-05   1.07903542e-02   6.73824650e-02\n",
      "    8.06356896e-02   3.87565456e-02   1.49090925e-02]]\n"
     ]
    }
   ],
   "source": [
    "keras_probs = keras.predict_proba(X_test, verbose=0)\n",
    "print keras_probs[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  3.48426832e-03   1.16913358e-01   9.31749807e-04   7.18774146e-04\n",
      "    6.29154016e-02   6.94968199e-03   3.52990018e-03   4.30306703e-02\n",
      "    5.76666126e-03   2.31244903e-03   6.48691587e-04   1.13350282e-03\n",
      "    1.73865042e-02   1.98705418e-02   4.62154143e-04   4.85318963e-03\n",
      "    1.05479662e-01   2.92981061e-03   1.51546359e-03   5.71302423e-02\n",
      "    8.23094002e-02   1.23750257e-01   1.24792768e-04   6.28301369e-03\n",
      "    7.41351862e-03   2.54724525e-02   4.69753348e-03   1.85440104e-02\n",
      "    8.01852512e-03   4.87338186e-04   5.68227027e-03   1.02860750e-03\n",
      "    4.56800496e-02   7.14054225e-05   1.07903542e-02   6.73824650e-02\n",
      "    8.06356896e-02   3.87565456e-02   1.49090925e-02]]\n"
     ]
    }
   ],
   "source": [
    "print keras_probs[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job completed\n"
     ]
    }
   ],
   "source": [
    "import gzip\n",
    "with gzip.open('submission-matrix-keras-4.csv.gz', 'wb') as f:\n",
    "    out = csv.writer(f, lineterminator='\\n')\n",
    "    out.writerow(['Id'] + list(np.unique(crime_labels)))\n",
    "    \n",
    "    for i, prob in enumerate(keras_probs):\n",
    "        out.writerow([i] + list(prob))\n",
    "print \"Job completed\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  7.]\n",
      " [ 37.]\n",
      " [ 20.]\n",
      " [ 16.]\n",
      " [ 20.]]\n"
     ]
    }
   ],
   "source": [
    "print y[:5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pgundugola\\Downloads\\WinPython-64bit-2.7.10.2\\python-2.7.10.amd64\\lib\\site-packages\\sklearn\\utils\\validation.py:449: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\pgundugola\\Downloads\\WinPython-64bit-2.7.10.2\\python-2.7.10.amd64\\lib\\site-packages\\sklearn\\metrics\\classification.py:676: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=\"f1_weighted\" instead of scoring=\"f1\".\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Results\n",
      "--------------------\n",
      "Accuracy:  0.22843\n",
      "F1 Score:  0.317925008596\n"
     ]
    }
   ],
   "source": [
    "mini_testX, mini_test_labels = X[800000:], y[800000:]\n",
    "mini_devX, mini_dev_labels = X[700000:800000], y[700000:800000]\n",
    "mini_trainX, mini_train_labels = X[:200000], y[:200000]\n",
    "\n",
    "lr = LogisticRegression(C=0.01)\n",
    "lr.fit(mini_trainX, mini_train_labels)\n",
    "lr_probs = lr.predict_proba(mini_testX)\n",
    "lr_accuracy = lr.score(mini_devX, mini_dev_labels)\n",
    "#lr_log_loss = log_loss(mini_test_labels, lr.predict_proba(mini_testX))\n",
    "f1_score = metrics.f1_score(lr.predict(mini_testX), mini_test_labels)\n",
    "\n",
    "print \"Logistic Regression Results\"\n",
    "print \"--------------------\"\n",
    "print \"Accuracy: \", lr_accuracy\n",
    "#print \"Log Loss: \", lr_log_loss\n",
    "print \"F1 Score: \", f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'Category': 'WARRANTS', 'DayOfWeek': 'Wednesday', 'Dates': '2015-05-13 23:53:00', 'Descript': 'WARRANT ARREST', 'PdDistrict': 'NORTHERN', 'Address': 'OAK ST / LAGUNA ST', 'Y': '37.7745985956747', 'X': '-122.425891675136', 'Resolution': 'ARREST, BOOKED'}]\n"
     ]
    }
   ],
   "source": [
    "def get_pre_data(fn):\n",
    "  data = []\n",
    "  with open(fn) as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    data = [row for row in reader]\n",
    "  return data\n",
    "raw_train = get_pre_data('../train.csv')\n",
    "print raw_train[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_prep_data(fn):\n",
    "  data = []\n",
    "  with open(fn) as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    full_data = []\n",
    "    for row in reader:\n",
    "        data = {}\n",
    "        data['Category'] = row['Category']\n",
    "        data['DayOfWeek'] = row['DayOfWeek']\n",
    "        data['Dates'] = row['Dates']\n",
    "        data['Descript'] = row['Descript']\n",
    "        data['PdDistrict'] = row['PdDistrict']\n",
    "        data['Address'] = row['Address']\n",
    "        data['Longitude'] \n",
    "  return data\n",
    "raw_train = get_prep_data('../train.csv')\n",
    "print raw_train[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Creating the clusters for X, Y Coordinates\n",
    "data_xy = np.array(data_df_orig[['X', 'Y']].values)\n",
    "km = KMeans(n_clusters=100)\n",
    "X_fit = km.fit(data_xy)\n",
    "y = km.labels_\n",
    "clusters = X_fit.labels_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
