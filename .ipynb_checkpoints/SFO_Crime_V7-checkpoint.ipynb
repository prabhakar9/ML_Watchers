{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# General Libraries\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "# SK-learn libraries for learning.\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.mixture import GMM\n",
    "from matplotlib.colors import LogNorm\n",
    "\n",
    "# SK-learn libraries for evaluation.\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# SK-learn libraries for feature extraction from text.\n",
    "from sklearn.feature_extraction.text import *\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.metrics import log_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the Crime data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Dates', 'Descript', 'DayOfWeek', 'PdDistrict', 'Resolution', 'Address', 'X', 'Y']\n",
      "[['2015-05-13 23:53:00', 'WARRANT ARREST', 'Wednesday', 'NORTHERN', 'ARREST, BOOKED', 'OAK ST / LAGUNA ST', '-122.425891675136', '37.7745985956747']]\n"
     ]
    }
   ],
   "source": [
    "crime_data = []\n",
    "crime_labels = []\n",
    "with open(\"../train.csv\", \"rb\") as csvfile:\n",
    "    reader = csv.reader(csvfile, delimiter=',')\n",
    "    first_row = True\n",
    "    for row in reader:\n",
    "        data = [y for x, y in enumerate(row) if x != 1]\n",
    "        name = [y for x, y in enumerate(row) if x == 1]\n",
    "        \n",
    "        if first_row:\n",
    "            first_row = False\n",
    "            feature_names = data\n",
    "            print feature_names\n",
    "        else:\n",
    "            crime_data.append(data)\n",
    "            crime_labels.append(name)\n",
    "\n",
    "print crime_data[:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Divide the Crime data into Train data, test data, mini train data, and dev sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "378049\n",
      "Size of train data:  500000\n",
      "Size of dev data:  189024\n",
      "Size of test data:  884262\n"
     ]
    }
   ],
   "source": [
    "# Divide the Crime training set into train data, test data, dev data\n",
    "train_data, train_labels = crime_data[:500000], crime_labels[:500000]\n",
    "crime_test, crime_test_labels = crime_data[500000:], crime_labels[500000:]\n",
    "num_test = len(crime_test)\n",
    "print num_test\n",
    "dev_data, dev_labels = crime_test[:num_test/2], crime_test_labels[:num_test/2]\n",
    "testX_data, testX_labels = crime_test[num_test/2:], crime_test_labels[num_test/2:]\n",
    "mini_train_data, mini_train_labels = crime_data[200000:300000], crime_labels[200000:300000]\n",
    "\n",
    "print \"Size of train data: \", len(train_data)\n",
    "print \"Size of dev data: \", len(dev_data)\n",
    "print \"Size of test data: \", len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['2015-05-13 23:53:00' 'WARRANT ARREST' 'Wednesday' 'NORTHERN'\n",
      "  'ARREST, BOOKED' 'OAK ST / LAGUNA ST' -122.425891675136 37.7745985956747]]\n"
     ]
    }
   ],
   "source": [
    "data_df = pd.read_csv(\"../train.csv\")\n",
    "crime_data = np.array(data_df[['Dates', 'Descript', 'DayOfWeek', 'PdDistrict', 'Resolution', 'Address', 'X', 'Y']].values)\n",
    "crime_labels = np.array(data_df[['Category']].values.ravel())\n",
    "print crime_data[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0L '2015-05-10 23:59:00' 'Sunday' 'BAYVIEW' '2000 Block of THOMAS AV'\n",
      "  -122.39958770418998 37.7350510103906]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['Id', 'Dates', 'DayOfWeek', 'PdDistrict', 'Address', 'X', 'Y'], dtype=object)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data_df = pd.read_csv(\"../test.csv\")\n",
    "test_data = np.array(test_data_df.values)\n",
    "print test_data[:1]\n",
    "test_data_df.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_xy = np.array(data_df[['X', 'Y']].values)\n",
    "test_xy = np.array(test_data_df[['X', 'Y']].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0.   0.   0.   0.   8.  14.  14.  15.  22.  27.  27.  30.  38.   0.  15.\n",
      "  15.  38.  14.  15.  38.  14.  14.  38.  38.   3.  35.   3.  10.  30.  30.]\n"
     ]
    }
   ],
   "source": [
    "print cl_index[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  2.   2.   2.   2.   5.  10.  10.  18.  20.  29.  29.  30.  35.   2.  18.\n",
      "  18.  35.  10.  18.  35.  10.  10.  35.  35.   3.  38.   3.  13.  30.  30.]\n",
      "             X          Y  index\n",
      "35 -122.391846  37.778125     35\n",
      "40 -122.405295  37.786307     40\n",
      "41 -122.422128  37.773033     41\n",
      "52 -122.403285  37.787306     52\n",
      "53 -122.397616  37.777392     53\n",
      "54 -122.420347  37.771769     54\n",
      "55 -122.399802  37.788007     55\n",
      "57 -122.411279  37.771579     57\n",
      "63 -122.395098  37.794059     63\n",
      "64 -122.411637  37.771787     64\n"
     ]
    }
   ],
   "source": [
    "districts = ['NORTHERN', 'PARK', 'INGLESIDE', 'BAYVIEW', 'RICHMOND', 'CENTRAL', 'TARAVAL', 'TENDERLOIN', 'MISSION', 'SOUTHERN']\n",
    "\n",
    "cl_index = np.zeros(data_xy.shape[0])\n",
    "for i in range(len(districts)):\n",
    "    data_cl_df = data_df[data_df['PdDistrict'] == districts[i]][['X', 'Y']]\n",
    "    data_cl_xy = np.array(data_df[['X', 'Y']].values)\n",
    "    index = data_df[data_df['PdDistrict'] == districts[i]].index.tolist()\n",
    "    data_cl_df['index'] = index\n",
    "    km = KMeans(n_clusters=5)\n",
    "    X_kfit = km.fit(data_cl_xy)\n",
    "    clusters = X_kfit.labels_ + i*5\n",
    "    cl_index[index] = clusters\n",
    "    \n",
    "print cl_index[:30]\n",
    "\n",
    "print data_cl_df[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(878049L,)\n"
     ]
    }
   ],
   "source": [
    "print cl_index.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 18.  18.   0.  14.  14.  34.  13.  13.  40.  25.  13.  40.  40.  25.  25.\n",
      "  35.  34.  13.   0.   0.   0.  40.  25.  13.   0.  22.   0.  45.  14.  32.]\n",
      "             X          Y  index\n",
      "27 -122.403405  37.775421     27\n",
      "36 -122.410939  37.779212     36\n",
      "47 -122.410294  37.782231     47\n",
      "50 -122.405619  37.778505     50\n",
      "56 -122.413161  37.777457     56\n",
      "64 -122.406521  37.785063     64\n",
      "65 -122.403405  37.775421     65\n",
      "66 -122.403405  37.775421     66\n",
      "69 -122.407634  37.784189     69\n",
      "70 -122.403405  37.775421     70\n"
     ]
    }
   ],
   "source": [
    "clt_index = np.zeros(test_xy.shape[0])\n",
    "for i in range(len(districts)):\n",
    "    test_cl_df = test_data_df[test_data_df['PdDistrict'] == districts[i]][['X', 'Y']]\n",
    "    test_cl_xy = np.array(test_cl_df[['X','Y']].values)\n",
    "    index = test_data_df[test_data_df['PdDistrict'] == districts[i]].index.tolist()\n",
    "    test_cl_df['index'] = index\n",
    "    test_clusters = km.predict(test_cl_xy) + i*5\n",
    "    \n",
    "    clt_index[index] = test_clusters\n",
    "print clt_index[:30]\n",
    "print test_cl_df[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next section has all UTIL functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(878049L,)\n",
      "[  2.   2.   2.   2.   5.  10.  10.  18.  20.  29.  29.  30.  35.   2.  18.\n",
      "  18.  35.  10.  18.  35.]\n",
      "(884262L,)\n",
      "[ 18.  18.   0.  14.  14.  34.  13.  13.  40.  25.  13.  40.  40.  25.  25.\n",
      "  35.  34.  13.   0.   0.]\n"
     ]
    }
   ],
   "source": [
    "print cl_index.shape\n",
    "print cl_index[:20]\n",
    "print clt_index.shape\n",
    "print clt_index[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2015-05-13 23:53:00\n",
      "[2015, 5, 13, 1433.0, 23, 'Night']\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "def convert_date(date_val):\n",
    "    date_list = []\n",
    "    date, time = date_val.split()\n",
    "    date_list.append(int(date.split('-')[0]))\n",
    "    date_list.append(int(date.split('-')[1]))\n",
    "    date_list.append(int(date.split('-')[2]))\n",
    "    time_in_min = int(time.split(':')[0]) * 60.0 + int(time.split(':')[1])\n",
    "    date_list.append(time_in_min)\n",
    "    date_list.append(int(time.split(':')[0]))\n",
    "    time_hour = int(time.split(':')[0])\n",
    "    if time_hour < 6:\n",
    "        time_of_day = 'Twilight'\n",
    "    elif time_hour < 12:\n",
    "        time_of_day = 'Morning'\n",
    "    elif time_hour < 18:\n",
    "        time_of_day = 'Afternoon'\n",
    "    else:\n",
    "        time_of_day = 'Night'\n",
    "    date_list.append(time_of_day)\n",
    "    return date_list\n",
    "\n",
    "print train_data[0][0]\n",
    "print convert_date(train_data[0][0])\n",
    "\n",
    "def find_mean_std(train_data, index):\n",
    "    data = []\n",
    "    for row in train_data:\n",
    "        data.append(float(row[index]))\n",
    "    \n",
    "    data_arr = np.array(data, dtype=np.float32)\n",
    "    print np.mean(data_arr)\n",
    "    data_dict = {}\n",
    "    data_dict['mean'] = np.mean(data_arr)\n",
    "    data_dict['std'] = np.std(data_arr)\n",
    "    return data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "date_arr = convert_date(train_data[0][0])\n",
    "year, month, day, time_in_min, hour, time_of_day = date_arr\n",
    "\n",
    "cyear = [(year < 2006) * 1, (year < 2008) * 1, (year < 2010) * 1, (year < 2012)*1, (year < 2015)*1, (year == 2015)*1]\n",
    "print cyear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialized Arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "districts = ['NORTHERN', 'PARK', 'INGLESIDE', 'BAYVIEW', 'RICHMOND', 'CENTRAL', 'TARAVAL', 'TENDERLOIN', 'MISSION', 'SOUTHERN']\n",
    "week_day = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "daytime = ['Twilight', 'Morning', 'Afternoon', 'Night']\n",
    "year_range = ['2003-2006', '2006-2009', '2009-2012', 'After 2015']\n",
    "crime_year = [2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015]\n",
    "crime_month = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
    "crime_day = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29 ,30 ,31]\n",
    "crime_hour = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Engineering - Section I:\n",
    "\n",
    "1) Crime Date - Normalization\n",
    "\n",
    "2) Longitude & Latitude - Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-122.423\n",
      "37.7675\n"
     ]
    }
   ],
   "source": [
    "''' Crime Date - Normalization '''\n",
    "\n",
    "year_data = []\n",
    "mon_data = []\n",
    "day_data = []\n",
    "time_data = []\n",
    "time_of_day_data = []\n",
    "for row in crime_data:\n",
    "    date_arr = convert_date(row[0])\n",
    "    year_data.append(float(date_arr[0]))\n",
    "    mon_data.append(float(date_arr[1]))\n",
    "    day_data.append(float(date_arr[2]))\n",
    "    time_data.append(float(date_arr[3]))\n",
    "    time_of_day_data.append(date_arr[5])\n",
    "                     \n",
    "\n",
    "year_arr = np.array(year_data, dtype=np.float32)\n",
    "mon_arr = np.array(mon_data, dtype=np.float32)\n",
    "day_arr = np.array(day_data, dtype=np.float32)\n",
    "time_arr = np.array(time_data, dtype=np.float32)\n",
    "time_of_day_arr = np.array(time_of_day_data)\n",
    "\n",
    "date_dict = {}\n",
    "date_dict['mean_year'] = np.mean(year_arr)\n",
    "date_dict['std_year'] = np.std(year_arr)                               \n",
    "date_dict['mean_mon'] = np.mean(mon_arr)\n",
    "date_dict['std_mon'] = np.std(mon_arr)\n",
    "date_dict['mean_day'] = np.mean(day_arr)\n",
    "date_dict['std_day'] = np.std(day_arr)\n",
    "date_dict['mean_time'] = np.mean(time_arr)\n",
    "date_dict['std_time'] = np.std(time_arr)\n",
    "\n",
    "\n",
    "''' Longitude & Latitude - Normalization '''\n",
    "long_dict = find_mean_std(train_data, 6)\n",
    "lat_dict = find_mean_std(train_data, 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_formatted_data(train_data):\n",
    "    format_data = []\n",
    "    count = 0\n",
    "    \n",
    "    for i in range(len(train_data)):\n",
    "        count += 1\n",
    "        data = []\n",
    "        row = train_data[i]\n",
    "        \n",
    "        ''' Data Preparation - Crime date '''\n",
    "        \n",
    "        #date_list = convert_date(row[0])\n",
    "        # [2015, 5, 13, 1433.0, 23, 'Night']\n",
    "        \n",
    "        date_arr = convert_date(row[0])\n",
    "        year, month, day, time_in_min, hour, time_of_day = date_arr\n",
    "        \n",
    "        norm_year = [1 if year == y else 0 for y in crime_year]\n",
    "        data.extend(norm_year)\n",
    "        \n",
    "        norm_month = [1 if month == m else 0 for m in crime_month]\n",
    "        data.extend(norm_month)\n",
    "        \n",
    "        norm_day = [1 if day == d else 0 for d in crime_day]\n",
    "        data.extend(norm_day)\n",
    "        \n",
    "        norm_hour = [1 if hour == h else 0 for h in crime_hour]\n",
    "        data.extend(norm_hour)\n",
    "        \n",
    "        # Time of the day preparation\n",
    "        day_time = [1 if time_of_day == td else 0 for td in daytime]\n",
    "        data.extend(day_time)\n",
    "        \n",
    "        \n",
    "        ''' Data Preparation - Day of the Week '''\n",
    "        wk_day = row[2]\n",
    "        \n",
    "        crime_week_day = [1 if wk_day == d else 0 for d in week_day] \n",
    "        data.extend(crime_week_day) # Normalized\n",
    "        \n",
    "        ''' Data Preparation - District '''\n",
    "        pdd = row[3]\n",
    "        pddistrict = [1 if pdd == d else 0 for d in districts] # Normalized\n",
    "        data.extend(pddistrict)\n",
    "        \n",
    "        ''' Data Preparation - Longitude & Latitude '''\n",
    "        longitude = float(row[6])\n",
    "        long_norm = (abs(longitude) - abs(long_dict['mean']))/long_dict['std'] # Normalized\n",
    "        latitude = float(row[7])\n",
    "        lat_norm = (abs(latitude) - abs(lat_dict['mean']))/lat_dict['std'] # Normalized\n",
    "        \n",
    "        data.append(long_norm)\n",
    "        data.append(lat_norm)\n",
    "        \n",
    "        #cluster_label = cl_index[i]\n",
    "        #data.append(cluster_label)\n",
    "        \n",
    "        ''' Data Preparation - Address '''\n",
    "        \n",
    "        address = row[5].lower()\n",
    "        addr = [1 if 'block' in address else 0] # Requires major improvement\n",
    "        data.extend(addr)\n",
    "        \n",
    "        # Explore external map source\n",
    "        \n",
    "        ''' Quantify the data '''\n",
    "        format_data.append(np.array(data, dtype=np.float32))           \n",
    "        \n",
    "    return format_data\n",
    "    \n",
    "    \n",
    "crimeX = np.array(get_formatted_data(crime_data), dtype=np.float32)\n",
    "print crimeX[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          1.          0.\n",
      "   0.          0.          0.          1.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          1.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          1.          0.          0.          0.          1.\n",
      "   0.          0.          1.          0.          0.          0.          0.\n",
      "   0.          1.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.6373859   0.16966379  5.          1.        ]]\n",
      "(878049L, 105L)\n"
     ]
    }
   ],
   "source": [
    "print crimeX[4:5]\n",
    "print crimeX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CrimeX Data shape:  (878049L, 105L)\n",
      "CrimeX Label shape:  (878049L,)\n"
     ]
    }
   ],
   "source": [
    "print \"CrimeX Data shape: \", crimeX.shape\n",
    "print \"CrimeX Label shape: \", np.array(crime_labels).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building the Logistic Regression model on the entire Crime Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 0.01}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pgundugola\\Downloads\\WinPython-64bit-2.7.10.2\\python-2.7.10.amd64\\lib\\site-packages\\sklearn\\cross_validation.py:417: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of labels for any class cannot be less than n_folds=3.\n",
      "  % (min_labels, self.n_folds)), Warning)\n"
     ]
    }
   ],
   "source": [
    "params = {'C':[0.001, 0.01, 0.1, 0.2, 1, 10, 100]}\n",
    "clf = GridSearchCV(LogisticRegression(penalty='l2'), params)\n",
    "clf.fit(crimeX[:10000], crime_labels[:10000])\n",
    "print clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed training with best C\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(penalty='l2', C=0.01, tol=0.01)\n",
    "lr.fit(crimeX, crime_labels)\n",
    "print \"Completed training with best C\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0L '2015-05-10 23:59:00' 'Sunday' 'BAYVIEW' '2000 Block of THOMAS AV'\n",
      "  -122.39958770418998 37.7350510103906]\n",
      " [1L '2015-05-10 23:51:00' 'Sunday' 'BAYVIEW' '3RD ST / REVERE AV'\n",
      "  -122.391522893042 37.7324323864471]\n",
      " [2L '2015-05-10 23:50:00' 'Sunday' 'NORTHERN' '2000 Block of GOUGH ST'\n",
      "  -122.426001954961 37.7922124386284]\n",
      " [3L '2015-05-10 23:45:00' 'Sunday' 'INGLESIDE' '4700 Block of MISSION ST'\n",
      "  -122.437393972517 37.7214120621391]\n",
      " [4L '2015-05-10 23:45:00' 'Sunday' 'INGLESIDE' '4700 Block of MISSION ST'\n",
      "  -122.437393972517 37.7214120621391]]\n"
     ]
    }
   ],
   "source": [
    "print test_data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_formatted_test_data(test_data):\n",
    "    format_data = []\n",
    "    count = 0\n",
    "    \n",
    "    for i in range(len(test_data)):\n",
    "        count += 1\n",
    "        data = []\n",
    "        \n",
    "        row = test_data[i]\n",
    "        \n",
    "        ''' Data Preparation - Date '''\n",
    "        date_arr = convert_date(row[1])\n",
    "        year, month, day, time_in_min, hour, time_of_day = date_arr\n",
    "        \n",
    "        norm_year = [1 if year == y else 0 for y in crime_year]\n",
    "        data.extend(norm_year)\n",
    "        \n",
    "        norm_month = [1 if month == m else 0 for m in crime_month]\n",
    "        data.extend(norm_month)\n",
    "        \n",
    "        norm_day = [1 if day == d else 0 for d in crime_day]\n",
    "        data.extend(norm_day)\n",
    "        \n",
    "        norm_hour = [1 if hour == h else 0 for h in crime_hour]\n",
    "        data.extend(norm_hour)\n",
    "        \n",
    "        # Time of the day preparation\n",
    "        day_time = [1 if time_of_day == td else 0 for td in daytime]\n",
    "        data.extend(day_time)\n",
    "        \n",
    "        ''' Data Preparation - Day of the Week '''\n",
    "        wk_day = row[2]\n",
    "        \n",
    "        crime_week_day = [1 if wk_day == d else 0 for d in week_day] \n",
    "        data.extend(crime_week_day) # Normalized\n",
    "        \n",
    "        pdd = row[3]\n",
    "        pddistrict = [1 if pdd == d else 0 for d in districts]\n",
    "        data.extend(pddistrict)\n",
    "        \n",
    "        longitude = float(row[5])\n",
    "        long_norm = (abs(longitude) - abs(long_dict['mean']))/long_dict['std']\n",
    "        latitude = float(row[6])\n",
    "        lat_norm = (abs(latitude) - abs(lat_dict['mean']))/lat_dict['std']\n",
    "        data.append(long_norm)\n",
    "        data.append(lat_norm)\n",
    "        \n",
    "        #test_cluster_label = clt_index[i]\n",
    "        #data.append(test_cluster_label)\n",
    "        \n",
    "        if 'block' in row[4].lower():\n",
    "            address = 1\n",
    "        else:\n",
    "            address = 0\n",
    "        data.append(address)\n",
    "        format_data.append(np.array(data, dtype=np.float32))           \n",
    "        \n",
    "    return format_data\n",
    "    \n",
    "    \n",
    "\n",
    "testX = np.array(get_formatted_test_data(test_data), dtype=np.float32)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(884262L, 105L)\n"
     ]
    }
   ],
   "source": [
    "print testX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(884262L, 39L)\n",
      "[[  5.76719885e-03   1.38896575e-01   4.74332077e-04   9.72481207e-04\n",
      "    4.10956684e-02   2.23499731e-03   2.96142556e-03   2.67066516e-02\n",
      "    6.04037428e-03   7.58891098e-04   4.81646263e-04   9.02829216e-04\n",
      "    2.67756690e-03   7.90894597e-03   4.12690830e-04   5.20545838e-03\n",
      "    1.53400514e-01   2.01001208e-03   4.43977954e-04   5.77654522e-02\n",
      "    7.90715702e-02   1.12325078e-01   4.65338712e-06   7.79030731e-04\n",
      "    3.34635433e-03   2.94284835e-02   3.29775434e-03   2.94264581e-02\n",
      "    6.59167111e-03   4.10996069e-04   4.43699061e-03   8.25669814e-04\n",
      "    3.45173831e-02   5.30678249e-09   6.47385903e-03   8.88071084e-02\n",
      "    8.54435409e-02   3.58083515e-02   2.18873530e-02]]\n"
     ]
    }
   ],
   "source": [
    "print probs.shape\n",
    "print probs[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(884262L, 39L)\n",
      "[[ 0.00598014  0.14235451  0.00039644  0.00095056  0.04091489  0.00200755\n",
      "   0.00284076  0.01751568  0.0054245   0.00065421  0.00042781  0.00088631\n",
      "   0.00179967  0.00680445  0.00042315  0.00505384  0.15562875  0.00179362\n",
      "   0.00036821  0.05546664  0.08141095  0.11349672  0.00025786  0.00034429\n",
      "   0.00303172  0.03292385  0.00276192  0.0313208   0.00717204  0.00044138\n",
      "   0.00499776  0.0007008   0.03670595  0.00025392  0.00626717  0.08456407\n",
      "   0.08833596  0.03474202  0.02257913]]\n"
     ]
    }
   ],
   "source": [
    "print probs.shape\n",
    "print probs[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "probs = lr.predict_proba(testX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job completed\n"
     ]
    }
   ],
   "source": [
    "import gzip\n",
    "with gzip.open('submission-matrix-4.csv.gz', 'wb') as f:\n",
    "    out = csv.writer(f, lineterminator='\\n')\n",
    "    out.writerow(['Id'] + list(np.unique(crime_labels)))\n",
    "    \n",
    "    for i, prob in enumerate(probs):\n",
    "        out.writerow([i] + list(prob))\n",
    "print \"Job completed\"     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total variance:  605.599814609\n",
      "[  3.63278569e+02   2.35078616e+02   1.00383855e+00   3.82392099e-01\n",
      "   2.83381964e-01   2.10347120e-01   1.97926614e-01   1.52390093e-01\n",
      "   1.50716336e-01   1.45815841e-01   1.42848013e-01   1.42457516e-01\n",
      "   1.39609921e-01   1.32438055e-01   1.13334640e-01   1.00798777e-01\n",
      "   9.68461754e-02   9.47954730e-02   9.29942762e-02   9.22755752e-02\n",
      "   9.12678902e-02   8.91869271e-02   8.58807513e-02   8.54009500e-02\n",
      "   8.49630651e-02   8.43242821e-02   8.36455223e-02   8.26095340e-02\n",
      "   8.25126662e-02   8.13376446e-02   8.12288718e-02   8.03732195e-02\n",
      "   7.99775536e-02   7.96967908e-02   7.90032433e-02   7.80847563e-02\n",
      "   7.72564729e-02   7.67170935e-02   7.56695562e-02   7.38380943e-02\n",
      "   6.65549106e-02   6.06995817e-02   6.02564384e-02   5.81009449e-02\n",
      "   5.62832475e-02   5.57260240e-02   5.48298032e-02   5.21171752e-02\n",
      "   5.15389757e-02   5.02277101e-02   4.96892338e-02   4.87185359e-02\n",
      "   4.33956464e-02   4.15586413e-02   4.11262551e-02   3.85114915e-02\n",
      "   3.54816607e-02   3.47359489e-02   3.43456038e-02   3.41301314e-02\n",
      "   3.40880637e-02   3.39993216e-02   3.39772556e-02   3.37231122e-02\n",
      "   3.36751031e-02   3.36208947e-02   3.35062651e-02   3.33407237e-02\n",
      "   3.29227805e-02   3.28542025e-02   3.25123624e-02   3.22181265e-02\n",
      "   3.20980027e-02   3.20122635e-02   3.19052992e-02   3.18173623e-02\n",
      "   3.15878695e-02   3.14232379e-02   3.12839064e-02   3.10014487e-02\n",
      "   3.08345926e-02   3.06021192e-02   3.04129169e-02   2.99864454e-02\n",
      "   2.73893500e-02   2.72334068e-02   2.49943696e-02   2.42459508e-02\n",
      "   1.85825069e-02   1.64496465e-02   1.63733032e-02   1.27391575e-02\n",
      "   1.16773218e-02   1.03124684e-02   1.01848394e-03   1.87556308e-09\n",
      "   1.45025495e-09   2.22196969e-10   1.25625822e-10   7.50510600e-11\n",
      "   7.23842472e-11   3.96304432e-11   2.30059342e-11   2.05924191e-11\n",
      "   1.00958191e-11]\n"
     ]
    }
   ],
   "source": [
    "pca = PCA()\n",
    "pca.fit_transform(crimeX)\n",
    "\n",
    "total_variance = pca.explained_variance_\n",
    "print \"Total variance: \", sum(total_variance)\n",
    "print total_variance\n",
    "print total_variance.shape\n",
    "print sum(total_variance[:30])\n",
    "print sum(total_variance[:3]/sum(total_variance))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.989698162338\n"
     ]
    }
   ],
   "source": [
    "print sum(total_variance[:2]/sum(total_variance))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(crimeX)\n",
    "Y_pca = crime_labels\n",
    "testX_pca = pca.transform(testX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_pca shape:  (878049L, 2L)\n",
      "testX_pca shape:  (884262L, 2L)\n",
      "0.988040568447\n"
     ]
    }
   ],
   "source": [
    "print \"X_pca shape: \", X_pca.shape\n",
    "print \"testX_pca shape: \", testX_pca.shape\n",
    "\n",
    "print sum(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA Logistic Regression completed\n"
     ]
    }
   ],
   "source": [
    "lrclf = LogisticRegression(penalty='l2', C=0.01, tol=0.01)\n",
    "lrclf.fit(X_pca, crime_labels)\n",
    "print \"PCA Logistic Regression completed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pca_probs = lrclf.predict_proba(testX_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(884262L, 39L)\n",
      "[[ 0.00304101  0.08539971  0.0011747   0.0011037   0.04450911  0.0046615\n",
      "   0.00322591  0.04795087  0.00463137  0.00185098  0.00102443  0.00135893\n",
      "   0.01172262  0.01810029  0.00091433  0.00380404  0.18795307  0.00235347\n",
      "   0.00167195  0.04590361  0.09597821  0.13543699  0.00079399  0.00780363\n",
      "   0.00642414  0.02619668  0.00430989  0.01575595  0.00517183  0.00091537\n",
      "   0.00539364  0.0013039   0.0352624   0.00078     0.0076642   0.05723957\n",
      "   0.0662368   0.04145146  0.01352575]]\n"
     ]
    }
   ],
   "source": [
    "print pca_probs.shape\n",
    "print pca_probs[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(884262L, 39L)\n",
      "[[ 0.00389724  0.09271721  0.00094632  0.00116931  0.03689604  0.00490239\n",
      "   0.00343383  0.04913983  0.00555169  0.00147622  0.00088461  0.0012982\n",
      "   0.00595283  0.01143436  0.0009087   0.0041626   0.19629199  0.00224684\n",
      "   0.0019071   0.03851902  0.07585033  0.1433693   0.00078098  0.01506296\n",
      "   0.00637133  0.03514683  0.00336287  0.01655724  0.00491559  0.00087532\n",
      "   0.00518267  0.00108951  0.02776805  0.00077764  0.00789536  0.06078678\n",
      "   0.06726688  0.04589369  0.01731032]]\n"
     ]
    }
   ],
   "source": [
    "print pca_probs.shape\n",
    "print pca_probs[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Boosting Completed\n"
     ]
    }
   ],
   "source": [
    "gclf = GradientBoostingClassifier()\n",
    "gclf.fit(crimeX, crime_labels)\n",
    "print \"Gradient Boosting Completed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "print \"hello\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job completed\n"
     ]
    }
   ],
   "source": [
    "import gzip\n",
    "with gzip.open('submission-matrix-pca-1.csv.gz', 'wb') as f:\n",
    "    out = csv.writer(f, lineterminator='\\n')\n",
    "    out.writerow(['Id'] + list(np.unique(crime_labels)))\n",
    "    \n",
    "    for i, prob in enumerate(pca_probs):\n",
    "        out.writerow([i] + list(prob))\n",
    "print \"Job completed\"   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(884262L, 39L)\n",
      "[[  3.02440836e-03   1.43862674e-01   2.03561777e-04   1.08661515e-03\n",
      "    4.46352994e-02   1.79431247e-03   4.26209288e-03   2.22077394e-02\n",
      "    4.31961594e-03   8.31534882e-04   2.49473397e-04   1.01547540e-03\n",
      "    3.65270688e-03   8.62011288e-03   1.90901827e-04   4.25244762e-03\n",
      "    1.29711833e-01   1.30613090e-03   3.64729688e-04   6.75900961e-02\n",
      "    7.56290451e-02   1.03543899e-01   1.68236500e-05   5.60963927e-04\n",
      "    1.56161420e-03   3.96501576e-02   4.48375028e-03   2.18597194e-02\n",
      "    5.38789645e-03   2.49638507e-04   4.96575344e-03   4.87584338e-04\n",
      "    4.72745625e-02   3.67647530e-06   6.41916468e-03   9.32536637e-02\n",
      "    9.39834953e-02   3.89940423e-02   1.84927866e-02]]\n"
     ]
    }
   ],
   "source": [
    "print gclf_probs.shape\n",
    "print gclf_probs[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Training Completed\n"
     ]
    }
   ],
   "source": [
    "clf = RandomForestClassifier(n_estimators=25)\n",
    "clf.fit(crimeX, crime_labels)\n",
    "print \"Random Forest Training Completed\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.882141746499\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\metrics\\classification.py:676: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=\"f1_weighted\" instead of scoring=\"f1\".\n",
      "  sample_weight=sample_weight)\n"
     ]
    }
   ],
   "source": [
    "mini_testX, mini_test_labels = testX[:10000], test_labels[:10000]\n",
    "f1_score = metrics.f1_score(clf.predict(mini_testX), mini_test_labels)\n",
    "print f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(884262L, 39L)\n",
      "[[ 0.          0.18        0.          0.          0.01333333  0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.48        0.          0.          0.03        0.07\n",
      "   0.02        0.          0.          0.          0.02        0.          0.\n",
      "   0.          0.          0.          0.          0.04        0.          0.\n",
      "   0.04        0.08        0.02666667  0.        ]]\n"
     ]
    }
   ],
   "source": [
    "clf_probs = clf.predict_proba(testX)\n",
    "print clf_probs.shape\n",
    "print clf_probs[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job completed\n"
     ]
    }
   ],
   "source": [
    "import gzip\n",
    "with gzip.open('submission-matrix.csv.gz', 'wb') as f:\n",
    "    out = csv.writer(f, lineterminator='\\n')\n",
    "    out.writerow(['Id'] + list(np.unique(crime_labels)))\n",
    "    \n",
    "    for i, prob in enumerate(clf_probs):\n",
    "        out.writerow([i] + list(prob))\n",
    "print \"Job completed\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_xy = np.array(data_df_orig[['X', 'Y']].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "km = KMeans(n_clusters=20)\n",
    "X_fit = km.fit(data_xy)\n",
    "y = km.labels_\n",
    "clusters = X_fit.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(878049L,)\n",
      "(878049L,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([13, 13, 27, 27, 52])"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print y.shape\n",
    "print clusters.shape\n",
    "clusters[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x89d16ef0>"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAEACAYAAACznAEdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x3ac0ff98>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(data_xy[:,0], data_xy[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BernoulliNB Results\n",
      "--------------------\n",
      "Accuracy:  0.08441\n",
      "F1 Score:  0.0797534912703\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\metrics\\classification.py:676: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=\"f1_weighted\" instead of scoring=\"f1\".\n",
      "  sample_weight=sample_weight)\n"
     ]
    }
   ],
   "source": [
    "mini_testX, mini_test_labels = crimeX[800000:], crime_labels[800000:]\n",
    "mini_devX, mini_dev_labels = crimeX[700000:800000], crime_labels[700000:800000]\n",
    "mini_trainX, mini_train_labels = crimeX[:200000], crime_labels[:200000]\n",
    "bnbb = BernoulliNB()\n",
    "bnbb.fit(mini_trainX, mini_train_labels)\n",
    "bnbb_probs = bnbb.predict_proba(mini_testX)\n",
    "bnbb_accuracy = bnbb.score(mini_devX, mini_dev_labels)\n",
    "# bnbb_log_loss = log_loss(mini_test_labels, bnbb_probs)\n",
    "f1_score = metrics.f1_score(bnbb.predict(mini_testX), mini_test_labels)\n",
    "\n",
    "\n",
    "print \"BernoulliNB Results\"\n",
    "print \"--------------------\"\n",
    "print \"Accuracy: \", bnbb_accuracy\n",
    "print \"F1 Score: \", f1_score\n",
    "#print \"Log Loss: \", bnbb_log_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Results\n",
      "--------------------\n",
      "Accuracy:  0.18925\n",
      "F1 Score:  0.270710483693\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\metrics\\classification.py:676: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=\"f1_weighted\" instead of scoring=\"f1\".\n",
      "  sample_weight=sample_weight)\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(C=0.01)\n",
    "lr.fit(mini_trainX, mini_train_labels)\n",
    "lr_probs = lr.predict_proba(mini_testX)\n",
    "lr_accuracy = lr.score(mini_devX, mini_dev_labels)\n",
    "#lr_log_loss = log_loss(mini_test_labels, lr.predict_proba(mini_testX))\n",
    "f1_score = metrics.f1_score(lr.predict(mini_testX), mini_test_labels)\n",
    "\n",
    "print \"Logistic Regression Results\"\n",
    "print \"--------------------\"\n",
    "print \"Accuracy: \", lr_accuracy\n",
    "#print \"Log Loss: \", lr_log_loss\n",
    "print \"F1 Score: \", f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Results\n",
      "--------------------\n",
      "Accuracy:  0.19248\n",
      "F1 Score:  0.262162080132\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\metrics\\classification.py:676: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=\"f1_weighted\" instead of scoring=\"f1\".\n",
      "  sample_weight=sample_weight)\n"
     ]
    }
   ],
   "source": [
    "rfclf = RandomForestClassifier(n_estimators=50)\n",
    "rfclf.fit(mini_trainX, mini_train_labels)\n",
    "rfclf_probs = rfclf.predict_proba(mini_testX)\n",
    "rfclf_accuracy = rfclf.score(mini_devX, mini_dev_labels)\n",
    "#lr_log_loss = log_loss(mini_test_labels, lr.predict_proba(mini_testX))\n",
    "f1_score = metrics.f1_score(rfclf.predict(mini_testX), mini_test_labels)\n",
    "\n",
    "print \"Random Forest Regression Results\"\n",
    "print \"--------------------\"\n",
    "print \"Accuracy: \", rfclf_accuracy\n",
    "#print \"Log Loss: \", lr_log_loss\n",
    "print \"F1 Score: \", f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
