{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# General Libraries\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "# SK-learn libraries for learning.\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.mixture import GMM\n",
    "from matplotlib.colors import LogNorm\n",
    "\n",
    "# SK-learn libraries for evaluation.\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# SK-learn libraries for feature extraction from text.\n",
    "from sklearn.feature_extraction.text import *\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "# Neural Networks\n",
    "from lasagne import layers\n",
    "from nolearn.lasagne import NeuralNet\n",
    "from lasagne import nonlinearities\n",
    "from lasagne.updates import nesterov_momentum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the Crime data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Dates', 'Descript', 'DayOfWeek', 'PdDistrict', 'Resolution', 'Address', 'X', 'Y']\n",
      "[['2015-05-13 23:53:00', 'WARRANT ARREST', 'Wednesday', 'NORTHERN', 'ARREST, BOOKED', 'OAK ST / LAGUNA ST', '-122.425891675136', '37.7745985956747']]\n"
     ]
    }
   ],
   "source": [
    "crime_data = []\n",
    "crime_labels = []\n",
    "with open(\"../train.csv\", \"rb\") as csvfile:\n",
    "    reader = csv.reader(csvfile, delimiter=',')\n",
    "    first_row = True\n",
    "    for row in reader:\n",
    "        data = [y for x, y in enumerate(row) if x != 1]\n",
    "        name = [y for x, y in enumerate(row) if x == 1]\n",
    "        \n",
    "        if first_row:\n",
    "            first_row = False\n",
    "            feature_names = data\n",
    "            print feature_names\n",
    "        else:\n",
    "            crime_data.append(data)\n",
    "            crime_labels.append(name)\n",
    "\n",
    "print crime_data[:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Divide the Crime data into Train data, test data, mini train data, and dev sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "378049\n",
      "Size of train data:  500000\n",
      "Size of dev data:  189024\n",
      "Size of test data:  189025\n"
     ]
    }
   ],
   "source": [
    "# Divide the Crime training set into train data, test data, dev data\n",
    "train_data, train_labels = crime_data[:500000], crime_labels[:500000]\n",
    "crime_test, crime_test_labels = crime_data[500000:], crime_labels[500000:]\n",
    "num_test = len(crime_test)\n",
    "print num_test\n",
    "dev_data, dev_labels = crime_test[:num_test/2], crime_test_labels[:num_test/2]\n",
    "test_data, test_labels = crime_test[num_test/2:], crime_test_labels[num_test/2:]\n",
    "mini_train_data, mini_train_labels = crime_data[200000:300000], crime_labels[200000:300000]\n",
    "\n",
    "print \"Size of train data: \", len(train_data)\n",
    "print \"Size of dev data: \", len(dev_data)\n",
    "print \"Size of test data: \", len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['2015-05-13 23:53:00' 'WARRANT ARREST' 'Wednesday' 'NORTHERN'\n",
      "  'ARREST, BOOKED' 'OAK ST / LAGUNA ST' -122.425891675136 37.7745985956747]]\n"
     ]
    }
   ],
   "source": [
    "data_df = pd.read_csv(\"../train.csv\")\n",
    "data_df_orig = data_df.copy()\n",
    "data_df = data_df[data_df.Y != 90]\n",
    "crime_data = np.array(data_df[['Dates', 'Descript', 'DayOfWeek', 'PdDistrict', 'Resolution', 'Address', 'X', 'Y']].values)\n",
    "crime_labels = np.array(data_df[['Category']].values.ravel())\n",
    "print crime_data[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0L '2015-05-10 23:59:00' 'Sunday' 'BAYVIEW' '2000 Block of THOMAS AV'\n",
      "  -122.39958770418998 37.7350510103906]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['Id', 'Dates', 'DayOfWeek', 'PdDistrict', 'Address', 'X', 'Y'], dtype=object)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data_df = pd.read_csv(\"../test.csv\")\n",
    "test_data = np.array(test_data_df.values)\n",
    "print test_data[:1]\n",
    "test_data_df.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Creating the clusters for X, Y Coordinates\n",
    "data_xy = np.array(data_df_orig[['X', 'Y']].values)\n",
    "km = KMeans(n_clusters=100)\n",
    "X_fit = km.fit(data_xy)\n",
    "y = km.labels_\n",
    "clusters = X_fit.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_xy = np.array(test_data_df[['X', 'Y']].values)\n",
    "test_clusters = km.predict(test_xy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next section has all UTIL functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13 13 27 27 52 50 92 83 84 88 88 30  2  6 33 29  2 42 29  2]\n",
      "[33  0 64  7  7 95 42 11 31 96 47  5  5 10 88 81 34 50  6 27]\n"
     ]
    }
   ],
   "source": [
    "clusters.shape\n",
    "print clusters[:20]\n",
    "test_clusters.shape\n",
    "print test_clusters[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2015-05-13 23:53:00\n",
      "[2015, 5, 13, 1433.0, 23, 'Night']\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "def convert_date(date_val):\n",
    "    date_list = []\n",
    "    date, time = date_val.split()\n",
    "    date_list.append(int(date.split('-')[0]))\n",
    "    date_list.append(int(date.split('-')[1]))\n",
    "    date_list.append(int(date.split('-')[2]))\n",
    "    time_in_min = int(time.split(':')[0]) * 60.0 + int(time.split(':')[1])\n",
    "    date_list.append(time_in_min)\n",
    "    date_list.append(int(time.split(':')[0]))\n",
    "    time_hour = int(time.split(':')[0])\n",
    "    if time_hour < 6:\n",
    "        time_of_day = 'Twilight'\n",
    "    elif time_hour < 12:\n",
    "        time_of_day = 'Morning'\n",
    "    elif time_hour < 18:\n",
    "        time_of_day = 'Afternoon'\n",
    "    else:\n",
    "        time_of_day = 'Night'\n",
    "    date_list.append(time_of_day)\n",
    "    return date_list\n",
    "\n",
    "print train_data[0][0]\n",
    "print convert_date(train_data[0][0])\n",
    "\n",
    "def find_mean_std(train_data, index):\n",
    "    data = []\n",
    "    for row in train_data:\n",
    "        data.append(float(row[index]))\n",
    "    \n",
    "    data_arr = np.array(data, dtype=np.float32)\n",
    "    print np.mean(data_arr)\n",
    "    data_dict = {}\n",
    "    data_dict['mean'] = np.mean(data_arr)\n",
    "    data_dict['std'] = np.std(data_arr)\n",
    "    return data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "date_arr = convert_date(train_data[0][0])\n",
    "year, month, day, time_in_min, hour, time_of_day = date_arr\n",
    "\n",
    "cyear = [(year < 2006) * 1, (year < 2008) * 1, (year < 2010) * 1, (year < 2012)*1, (year < 2015)*1, (year == 2015)*1]\n",
    "print cyear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialized Arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "districts = ['NORTHERN', 'PARK', 'INGLESIDE', 'BAYVIEW', 'RICHMOND', 'CENTRAL', 'TARAVAL', 'TENDERLOIN', 'MISSION', 'SOUTHERN']\n",
    "week_day = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "daytime = ['Twilight', 'Morning', 'Afternoon', 'Night']\n",
    "year_range = ['2003-2006', '2006-2009', '2009-2012', 'After 2015']\n",
    "crime_year = [2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015]\n",
    "crime_month = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
    "crime_day = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29 ,30 ,31]\n",
    "crime_hour = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Engineering - Section I:\n",
    "\n",
    "1) Crime Date - Normalization\n",
    "\n",
    "2) Longitude & Latitude - Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-122.423\n",
      "37.7675\n"
     ]
    }
   ],
   "source": [
    "''' Crime Date - Normalization '''\n",
    "\n",
    "year_data = []\n",
    "mon_data = []\n",
    "day_data = []\n",
    "time_data = []\n",
    "time_of_day_data = []\n",
    "for row in crime_data:\n",
    "    date_arr = convert_date(row[0])\n",
    "    year_data.append(float(date_arr[0]))\n",
    "    mon_data.append(float(date_arr[1]))\n",
    "    day_data.append(float(date_arr[2]))\n",
    "    time_data.append(float(date_arr[3]))\n",
    "    time_of_day_data.append(date_arr[5])\n",
    "                     \n",
    "\n",
    "year_arr = np.array(year_data, dtype=np.float32)\n",
    "mon_arr = np.array(mon_data, dtype=np.float32)\n",
    "day_arr = np.array(day_data, dtype=np.float32)\n",
    "time_arr = np.array(time_data, dtype=np.float32)\n",
    "time_of_day_arr = np.array(time_of_day_data)\n",
    "\n",
    "date_dict = {}\n",
    "date_dict['mean_year'] = np.mean(year_arr)\n",
    "date_dict['std_year'] = np.std(year_arr)                               \n",
    "date_dict['mean_mon'] = np.mean(mon_arr)\n",
    "date_dict['std_mon'] = np.std(mon_arr)\n",
    "date_dict['mean_day'] = np.mean(day_arr)\n",
    "date_dict['std_day'] = np.std(day_arr)\n",
    "date_dict['mean_time'] = np.mean(time_arr)\n",
    "date_dict['std_time'] = np.std(time_arr)\n",
    "\n",
    "\n",
    "''' Longitude & Latitude - Normalization '''\n",
    "long_dict = find_mean_std(train_data, 6)\n",
    "lat_dict = find_mean_std(train_data, 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0.           0.           0.           0.           0.           0.\n",
      "    0.           0.           0.           0.           0.           0.\n",
      "    1.           0.           0.           0.           0.           1.\n",
      "    0.           0.           0.           0.           0.           0.\n",
      "    0.           0.           0.           0.           0.           0.\n",
      "    0.           0.           0.           0.           0.           0.\n",
      "    0.           1.           0.           0.           0.           0.\n",
      "    0.           0.           0.           0.           0.           0.\n",
      "    0.           0.           0.           0.           0.           0.\n",
      "    0.           0.           0.           0.           0.           0.\n",
      "    0.           0.           0.           0.           0.           0.\n",
      "    0.           0.           0.           0.           0.           0.\n",
      "    0.           0.           0.           0.           0.           0.\n",
      "    0.           1.           0.           0.           0.           1.\n",
      "    0.           0.           1.           0.           0.           0.\n",
      "    0.           1.           0.           0.           0.           0.\n",
      "    0.           0.           0.           0.           0.           0.12929684\n",
      "    0.29712132  13.           0.        ]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def get_formatted_data(train_data):\n",
    "    format_data = []\n",
    "    count = 0\n",
    "    \n",
    "    for i in range(len(train_data)):\n",
    "        count += 1\n",
    "        data = []\n",
    "        row = train_data[i]\n",
    "        \n",
    "        ''' Data Preparation - Crime date '''\n",
    "        \n",
    "        #date_list = convert_date(row[0])\n",
    "        # [2015, 5, 13, 1433.0, 23, 'Night']\n",
    "        \n",
    "        date_arr = convert_date(row[0])\n",
    "        year, month, day, time_in_min, hour, time_of_day = date_arr\n",
    "        \n",
    "        norm_year = [1 if year == y else 0 for y in crime_year]\n",
    "        data.extend(norm_year)\n",
    "        \n",
    "        norm_month = [1 if month == m else 0 for m in crime_month]\n",
    "        data.extend(norm_month)\n",
    "        \n",
    "        norm_day = [1 if day == d else 0 for d in crime_day]\n",
    "        data.extend(norm_day)\n",
    "        \n",
    "        norm_hour = [1 if hour == h else 0 for h in crime_hour]\n",
    "        data.extend(norm_hour)\n",
    "        \n",
    "        # Time of the day preparation\n",
    "        day_time = [1 if time_of_day == td else 0 for td in daytime]\n",
    "        data.extend(day_time)\n",
    "        \n",
    "        \n",
    "        ''' Data Preparation - Day of the Week '''\n",
    "        wk_day = row[2]\n",
    "        \n",
    "        crime_week_day = [1 if wk_day == d else 0 for d in week_day] \n",
    "        data.extend(crime_week_day) # Normalized\n",
    "        \n",
    "        ''' Data Preparation - District '''\n",
    "        pdd = row[3]\n",
    "        pddistrict = [1 if pdd == d else 0 for d in districts] # Normalized\n",
    "        data.extend(pddistrict)\n",
    "        \n",
    "        ''' Data Preparation - Longitude & Latitude '''\n",
    "        longitude = float(row[6])\n",
    "        long_norm = (abs(longitude) - abs(long_dict['mean']))/long_dict['std'] # Normalized\n",
    "        latitude = float(row[7])\n",
    "        lat_norm = (abs(latitude) - abs(lat_dict['mean']))/lat_dict['std'] # Normalized\n",
    "        \n",
    "        data.append(long_norm)\n",
    "        data.append(lat_norm)\n",
    "        \n",
    "        cluster_label = clusters[i]\n",
    "        data.append(cluster_label)\n",
    "        \n",
    "        ''' Data Preparation - Address '''\n",
    "        \n",
    "        address = row[5].lower()\n",
    "        addr = [1 if 'block' in address else 0] # Requires major improvement\n",
    "        data.extend(addr)\n",
    "        \n",
    "        # Explore external map source\n",
    "        \n",
    "        ''' Quantify the data '''\n",
    "        format_data.append(np.array(data, dtype=np.float32))           \n",
    "        \n",
    "    return format_data\n",
    "    \n",
    "    \n",
    "crimeX = np.array(get_formatted_data(crime_data), dtype=np.float32)\n",
    "print crimeX[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(877982L, 105L)\n"
     ]
    }
   ],
   "source": [
    "print crimeX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CrimeX Data shape:  (877982L, 103L)\n",
      "CrimeX Label shape:  (877982L,)\n"
     ]
    }
   ],
   "source": [
    "print \"CrimeX Data shape: \", crimeX.shape\n",
    "print \"CrimeX Label shape: \", np.array(crime_labels).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building the Logistic Regression model on the entire Crime Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 0.01}\n"
     ]
    }
   ],
   "source": [
    "params = {'C':[0.001, 0.01, 0.1, 0.2, 1, 10, 100]}\n",
    "clf = GridSearchCV(LogisticRegression(penalty='l2'), params)\n",
    "clf.fit(crimeX[:10000], crime_labels[:10000])\n",
    "print clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed training with best C\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(C=0.01)\n",
    "lr.fit(crimeX, crime_labels)\n",
    "print \"Completed training with best C\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "print 'hello'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_formatted_test_data(test_data):\n",
    "    format_data = []\n",
    "    count = 0\n",
    "    \n",
    "    for i in range(len(test_data)):\n",
    "        count += 1\n",
    "        data = []\n",
    "        \n",
    "        row = test_data[i]\n",
    "        \n",
    "        ''' Data Preparation - Date '''\n",
    "        date_arr = convert_date(row[1])\n",
    "        year, month, day, time_in_min, hour, time_of_day = date_arr\n",
    "        \n",
    "        norm_year = [1 if year == y else 0 for y in crime_year]\n",
    "        data.extend(norm_year)\n",
    "        \n",
    "        norm_month = [1 if month == m else 0 for m in crime_month]\n",
    "        data.extend(norm_month)\n",
    "        \n",
    "        norm_day = [1 if day == d else 0 for d in crime_day]\n",
    "        data.extend(norm_day)\n",
    "        \n",
    "        norm_hour = [1 if hour == h else 0 for h in crime_hour]\n",
    "        data.extend(norm_hour)\n",
    "        \n",
    "        # Time of the day preparation\n",
    "        day_time = [1 if time_of_day == td else 0 for td in daytime]\n",
    "        data.extend(day_time)\n",
    "        \n",
    "        ''' Data Preparation - Day of the Week '''\n",
    "        wk_day = row[2]\n",
    "        \n",
    "        crime_week_day = [1 if wk_day == d else 0 for d in week_day] \n",
    "        data.extend(crime_week_day) # Normalized\n",
    "        \n",
    "        pdd = row[3]\n",
    "        pddistrict = [1 if pdd == d else 0 for d in districts]\n",
    "        data.extend(pddistrict)\n",
    "        \n",
    "        longitude = float(row[5])\n",
    "        long_norm = (abs(longitude) - abs(long_dict['mean']))/long_dict['std']\n",
    "        latitude = float(row[6])\n",
    "        lat_norm = (abs(latitude) - abs(lat_dict['mean']))/lat_dict['std']\n",
    "        data.append(long_norm)\n",
    "        data.append(lat_norm)\n",
    "        \n",
    "        test_cluster_label = test_clusters[i]\n",
    "        data.append(test_cluster_label)\n",
    "        \n",
    "        if 'block' in row[4].lower():\n",
    "            address = 1\n",
    "        else:\n",
    "            address = 0\n",
    "        data.append(address)\n",
    "        format_data.append(np.array(data, dtype=np.float32))           \n",
    "        \n",
    "    return format_data\n",
    "    \n",
    "    \n",
    "\n",
    "testX = np.array(get_formatted_test_data(test_data), dtype=np.float32)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(884262L, 105L)\n"
     ]
    }
   ],
   "source": [
    "print testX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(884262L, 39L)\n",
      "[[  5.57542273e-03   1.38922168e-01   4.66945980e-04   8.00756923e-04\n",
      "    4.34877216e-02   1.92076750e-03   3.05288908e-03   2.39357842e-02\n",
      "    6.37970135e-03   8.32827327e-04   4.82125027e-04   8.85237292e-04\n",
      "    1.72723858e-03   6.33979544e-03   2.65988642e-04   5.87162341e-03\n",
      "    1.45179349e-01   1.63756572e-03   3.02538547e-04   5.96974609e-02\n",
      "    8.69645921e-02   1.17018235e-01   1.38073731e-06   4.10919255e-04\n",
      "    4.01824873e-03   3.05324164e-02   2.42683643e-03   2.94659909e-02\n",
      "    6.67091621e-03   2.84041508e-04   6.79294021e-03   7.96747966e-04\n",
      "    3.82713773e-02   5.40703282e-10   6.97117204e-03   8.30055513e-02\n",
      "    7.75598006e-02   3.94632353e-02   2.15816904e-02]]\n"
     ]
    }
   ],
   "source": [
    "print probs.shape\n",
    "print probs[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(884262L, 39L)\n",
      "[[ 0.00598014  0.14235451  0.00039644  0.00095056  0.04091489  0.00200755\n",
      "   0.00284076  0.01751568  0.0054245   0.00065421  0.00042781  0.00088631\n",
      "   0.00179967  0.00680445  0.00042315  0.00505384  0.15562875  0.00179362\n",
      "   0.00036821  0.05546664  0.08141095  0.11349672  0.00025786  0.00034429\n",
      "   0.00303172  0.03292385  0.00276192  0.0313208   0.00717204  0.00044138\n",
      "   0.00499776  0.0007008   0.03670595  0.00025392  0.00626717  0.08456407\n",
      "   0.08833596  0.03474202  0.02257913]]\n"
     ]
    }
   ],
   "source": [
    "print probs.shape\n",
    "print probs[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "probs = lr.predict_proba(testX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job completed\n"
     ]
    }
   ],
   "source": [
    "import gzip\n",
    "with gzip.open('submission-matrix-3.csv.gz', 'wb') as f:\n",
    "    out = csv.writer(f, lineterminator='\\n')\n",
    "    out.writerow(['Id'] + list(np.unique(crime_labels)))\n",
    "    \n",
    "    for i, prob in enumerate(probs):\n",
    "        out.writerow([i] + list(prob))\n",
    "print \"Job completed\"     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Boosting Completed\n"
     ]
    }
   ],
   "source": [
    "gclf = GradientBoostingClassifier()\n",
    "gclf.fit(crimeX, crime_labels)\n",
    "print \"Gradient Boosting Completed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "print \"hello\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job completed\n"
     ]
    }
   ],
   "source": [
    "gclf_probs = gclf.predict_proba(testX)\n",
    "import gzip\n",
    "with gzip.open('submission-matrix-gclf.csv.gz', 'wb') as f:\n",
    "    out = csv.writer(f, lineterminator='\\n')\n",
    "    out.writerow(['Id'] + list(np.unique(crime_labels)))\n",
    "    \n",
    "    for i, prob in enumerate(gclf_probs):\n",
    "        out.writerow([i] + list(prob))\n",
    "print \"Job completed\"   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(884262L, 39L)\n",
      "[[  3.02440836e-03   1.43862674e-01   2.03561777e-04   1.08661515e-03\n",
      "    4.46352994e-02   1.79431247e-03   4.26209288e-03   2.22077394e-02\n",
      "    4.31961594e-03   8.31534882e-04   2.49473397e-04   1.01547540e-03\n",
      "    3.65270688e-03   8.62011288e-03   1.90901827e-04   4.25244762e-03\n",
      "    1.29711833e-01   1.30613090e-03   3.64729688e-04   6.75900961e-02\n",
      "    7.56290451e-02   1.03543899e-01   1.68236500e-05   5.60963927e-04\n",
      "    1.56161420e-03   3.96501576e-02   4.48375028e-03   2.18597194e-02\n",
      "    5.38789645e-03   2.49638507e-04   4.96575344e-03   4.87584338e-04\n",
      "    4.72745625e-02   3.67647530e-06   6.41916468e-03   9.32536637e-02\n",
      "    9.39834953e-02   3.89940423e-02   1.84927866e-02]]\n"
     ]
    }
   ],
   "source": [
    "print gclf_probs.shape\n",
    "print gclf_probs[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Training Completed\n"
     ]
    }
   ],
   "source": [
    "clf = RandomForestClassifier(n_estimators=25)\n",
    "clf.fit(crimeX, crime_labels)\n",
    "print \"Random Forest Training Completed\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.882141746499\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\metrics\\classification.py:676: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=\"f1_weighted\" instead of scoring=\"f1\".\n",
      "  sample_weight=sample_weight)\n"
     ]
    }
   ],
   "source": [
    "mini_testX, mini_test_labels = testX[:10000], test_labels[:10000]\n",
    "f1_score = metrics.f1_score(clf.predict(mini_testX), mini_test_labels)\n",
    "print f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(884262L, 39L)\n",
      "[[ 0.          0.18        0.          0.          0.01333333  0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.48        0.          0.          0.03        0.07\n",
      "   0.02        0.          0.          0.          0.02        0.          0.\n",
      "   0.          0.          0.          0.          0.04        0.          0.\n",
      "   0.04        0.08        0.02666667  0.        ]]\n"
     ]
    }
   ],
   "source": [
    "clf_probs = clf.predict_proba(testX)\n",
    "print clf_probs.shape\n",
    "print clf_probs[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job completed\n"
     ]
    }
   ],
   "source": [
    "import gzip\n",
    "with gzip.open('submission-matrix.csv.gz', 'wb') as f:\n",
    "    out = csv.writer(f, lineterminator='\\n')\n",
    "    out.writerow(['Id'] + list(np.unique(crime_labels)))\n",
    "    \n",
    "    for i, prob in enumerate(clf_probs):\n",
    "        out.writerow([i] + list(prob))\n",
    "print \"Job completed\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_xy = np.array(data_df_orig[['X', 'Y']].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "km = KMeans(n_clusters=20)\n",
    "X_fit = km.fit(data_xy)\n",
    "y = km.labels_\n",
    "clusters = X_fit.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(878049L,)\n",
      "(878049L,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([13, 13, 27, 27, 52])"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print y.shape\n",
    "print clusters.shape\n",
    "clusters[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x89d16ef0>"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAEACAYAAACznAEdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x3ac0ff98>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(data_xy[:,0], data_xy[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BernoulliNB Results\n",
      "--------------------\n",
      "Accuracy:  0.08441\n",
      "F1 Score:  0.0797534912703\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\metrics\\classification.py:676: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=\"f1_weighted\" instead of scoring=\"f1\".\n",
      "  sample_weight=sample_weight)\n"
     ]
    }
   ],
   "source": [
    "mini_testX, mini_test_labels = crimeX[800000:], crime_labels[800000:]\n",
    "mini_devX, mini_dev_labels = crimeX[700000:800000], crime_labels[700000:800000]\n",
    "mini_trainX, mini_train_labels = crimeX[:200000], crime_labels[:200000]\n",
    "bnbb = BernoulliNB()\n",
    "bnbb.fit(mini_trainX, mini_train_labels)\n",
    "bnbb_probs = bnbb.predict_proba(mini_testX)\n",
    "bnbb_accuracy = bnbb.score(mini_devX, mini_dev_labels)\n",
    "# bnbb_log_loss = log_loss(mini_test_labels, bnbb_probs)\n",
    "f1_score = metrics.f1_score(bnbb.predict(mini_testX), mini_test_labels)\n",
    "\n",
    "\n",
    "print \"BernoulliNB Results\"\n",
    "print \"--------------------\"\n",
    "print \"Accuracy: \", bnbb_accuracy\n",
    "print \"F1 Score: \", f1_score\n",
    "#print \"Log Loss: \", bnbb_log_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Results\n",
      "--------------------\n",
      "Accuracy:  0.18925\n",
      "F1 Score:  0.270710483693\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\metrics\\classification.py:676: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=\"f1_weighted\" instead of scoring=\"f1\".\n",
      "  sample_weight=sample_weight)\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(C=0.01)\n",
    "lr.fit(mini_trainX, mini_train_labels)\n",
    "lr_probs = lr.predict_proba(mini_testX)\n",
    "lr_accuracy = lr.score(mini_devX, mini_dev_labels)\n",
    "#lr_log_loss = log_loss(mini_test_labels, lr.predict_proba(mini_testX))\n",
    "f1_score = metrics.f1_score(lr.predict(mini_testX), mini_test_labels)\n",
    "\n",
    "print \"Logistic Regression Results\"\n",
    "print \"--------------------\"\n",
    "print \"Accuracy: \", lr_accuracy\n",
    "#print \"Log Loss: \", lr_log_loss\n",
    "print \"F1 Score: \", f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Results\n",
      "--------------------\n",
      "Accuracy:  0.19248\n",
      "F1 Score:  0.262162080132\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\metrics\\classification.py:676: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=\"f1_weighted\" instead of scoring=\"f1\".\n",
      "  sample_weight=sample_weight)\n"
     ]
    }
   ],
   "source": [
    "rfclf = RandomForestClassifier(n_estimators=50)\n",
    "rfclf.fit(mini_trainX, mini_train_labels)\n",
    "rfclf_probs = rfclf.predict_proba(mini_testX)\n",
    "rfclf_accuracy = rfclf.score(mini_devX, mini_dev_labels)\n",
    "#lr_log_loss = log_loss(mini_test_labels, lr.predict_proba(mini_testX))\n",
    "f1_score = metrics.f1_score(rfclf.predict(mini_testX), mini_test_labels)\n",
    "\n",
    "print \"Random Forest Regression Results\"\n",
    "print \"--------------------\"\n",
    "print \"Accuracy: \", rfclf_accuracy\n",
    "#print \"Log Loss: \", lr_log_loss\n",
    "print \"F1 Score: \", f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
